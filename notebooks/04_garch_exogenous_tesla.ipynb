{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this file we fit our model with the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.stats import norm, chi2\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox  \n",
    "\n",
    "from Garch import GARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series):\n",
    "    result = adfuller(series, autolag='AIC')  \n",
    "    print(f\"ADF Statistic: {result[0]}\")\n",
    "    print(f\"p-value: {result[1]}\")\n",
    "    print(\"Critical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "    if result[1] <= 0.05:\n",
    "        print(\" The series is stationary (reject H0)\")\n",
    "    else:\n",
    "        print(\" The series is non-stationary (fail to reject H0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ar_data(ar_coef, T):\n",
    "    res = [0]\n",
    "    for t in range(1,T):\n",
    "        res.append(ar_coef * res[-1] + random.gauss(0, 1))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_ratio_test(ll_null: float, ll_alternative: float, df: int):\n",
    "    if df <= 0:\n",
    "        raise ValueError(\"Degrees of freedom must be positive.\")\n",
    "    if ll_alternative < ll_null:\n",
    "        raise ValueError(\"ll_null should be smaller than ll_alternative.\")\n",
    "    \n",
    "    # Compute test statistic\n",
    "    lr_stat = 2 * (ll_alternative - ll_null)\n",
    "    \n",
    "    # Compute p-value\n",
    "    p_value = 1 - chi2.cdf(lr_stat, df)\n",
    "    \n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "1. S&P500 data\n",
    "2. Sentiment data (consisting of mean Positive, Negative and Neutral sentiment per day)\n",
    "3. VIX (for comparison purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load S&P data\n",
    "prices_df = pd.read_csv('../data/tesla_prices.csv')\n",
    "prices_df.index = pd.to_datetime(prices_df['Date'], format='%Y-%m-%d %H:%M:%S%z', utc=True).dt.date\n",
    "\n",
    "# Load Sentiment data\n",
    "sentiment_df = pd.read_csv('../data/tesla_sentiment_gpt_summarised.csv')\n",
    "sentiment_df.index = pd.DatetimeIndex(sentiment_df['adjusted_date'])\n",
    "\n",
    "# Load US interest rate data\n",
    "t_rates_df = pd.read_csv('../data/daily-treasury-rates.csv')\n",
    "t_rates_df.index = pd.DatetimeIndex(t_rates_df['Date'])\n",
    "t_rates_df = t_rates_df.drop(columns=['Date','Unnamed: 11',\t'Unnamed: 12', '26 WEEKS BANK DISCOUNT', '26 WEEKS COUPON EQUIVALENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjusted_date</th>\n",
       "      <th>mean_pos_sentiment</th>\n",
       "      <th>mean_neg_sentiment</th>\n",
       "      <th>mean_neutral_sentiment</th>\n",
       "      <th>mean_pos_preamble_sentiment</th>\n",
       "      <th>mean_neg_preamble_sentiment</th>\n",
       "      <th>mean_neutral_preamble_sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adjusted_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-01</th>\n",
       "      <td>1/1/24</td>\n",
       "      <td>0.257112</td>\n",
       "      <td>0.560906</td>\n",
       "      <td>0.181982</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.408570</td>\n",
       "      <td>0.110230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-10</th>\n",
       "      <td>1/10/21</td>\n",
       "      <td>0.144524</td>\n",
       "      <td>0.786026</td>\n",
       "      <td>0.069450</td>\n",
       "      <td>0.154926</td>\n",
       "      <td>0.794357</td>\n",
       "      <td>0.050718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-11</th>\n",
       "      <td>1/11/23</td>\n",
       "      <td>0.334139</td>\n",
       "      <td>0.621210</td>\n",
       "      <td>0.044651</td>\n",
       "      <td>0.353742</td>\n",
       "      <td>0.596856</td>\n",
       "      <td>0.049402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-12</th>\n",
       "      <td>1/12/20</td>\n",
       "      <td>0.450078</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.329879</td>\n",
       "      <td>0.695714</td>\n",
       "      <td>0.094795</td>\n",
       "      <td>0.209491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-12</th>\n",
       "      <td>1/12/23</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.904017</td>\n",
       "      <td>0.036183</td>\n",
       "      <td>0.159795</td>\n",
       "      <td>0.797815</td>\n",
       "      <td>0.042389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-08</th>\n",
       "      <td>9/8/22</td>\n",
       "      <td>0.026218</td>\n",
       "      <td>0.887631</td>\n",
       "      <td>0.086151</td>\n",
       "      <td>0.029794</td>\n",
       "      <td>0.879863</td>\n",
       "      <td>0.090342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-09</th>\n",
       "      <td>9/9/20</td>\n",
       "      <td>0.075703</td>\n",
       "      <td>0.606183</td>\n",
       "      <td>0.318115</td>\n",
       "      <td>0.077259</td>\n",
       "      <td>0.710299</td>\n",
       "      <td>0.212441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-09</th>\n",
       "      <td>9/9/21</td>\n",
       "      <td>0.285432</td>\n",
       "      <td>0.379544</td>\n",
       "      <td>0.335024</td>\n",
       "      <td>0.145217</td>\n",
       "      <td>0.586420</td>\n",
       "      <td>0.268363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-09</th>\n",
       "      <td>9/9/22</td>\n",
       "      <td>0.826727</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>0.154595</td>\n",
       "      <td>0.913103</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>0.074712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-09</th>\n",
       "      <td>9/9/23</td>\n",
       "      <td>0.474724</td>\n",
       "      <td>0.252565</td>\n",
       "      <td>0.272711</td>\n",
       "      <td>0.531944</td>\n",
       "      <td>0.197699</td>\n",
       "      <td>0.270358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>862 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              adjusted_date  mean_pos_sentiment  mean_neg_sentiment  \\\n",
       "adjusted_date                                                         \n",
       "2024-01-01           1/1/24            0.257112            0.560906   \n",
       "2021-01-10          1/10/21            0.144524            0.786026   \n",
       "2023-01-11          1/11/23            0.334139            0.621210   \n",
       "2020-01-12          1/12/20            0.450078            0.220043   \n",
       "2023-01-12          1/12/23            0.059800            0.904017   \n",
       "...                     ...                 ...                 ...   \n",
       "2022-09-08           9/8/22            0.026218            0.887631   \n",
       "2020-09-09           9/9/20            0.075703            0.606183   \n",
       "2021-09-09           9/9/21            0.285432            0.379544   \n",
       "2022-09-09           9/9/22            0.826727            0.018678   \n",
       "2023-09-09           9/9/23            0.474724            0.252565   \n",
       "\n",
       "               mean_neutral_sentiment  mean_pos_preamble_sentiment  \\\n",
       "adjusted_date                                                        \n",
       "2024-01-01                   0.181982                     0.481200   \n",
       "2021-01-10                   0.069450                     0.154926   \n",
       "2023-01-11                   0.044651                     0.353742   \n",
       "2020-01-12                   0.329879                     0.695714   \n",
       "2023-01-12                   0.036183                     0.159795   \n",
       "...                               ...                          ...   \n",
       "2022-09-08                   0.086151                     0.029794   \n",
       "2020-09-09                   0.318115                     0.077259   \n",
       "2021-09-09                   0.335024                     0.145217   \n",
       "2022-09-09                   0.154595                     0.913103   \n",
       "2023-09-09                   0.272711                     0.531944   \n",
       "\n",
       "               mean_neg_preamble_sentiment  mean_neutral_preamble_sentiment  \n",
       "adjusted_date                                                                \n",
       "2024-01-01                        0.408570                         0.110230  \n",
       "2021-01-10                        0.794357                         0.050718  \n",
       "2023-01-11                        0.596856                         0.049402  \n",
       "2020-01-12                        0.094795                         0.209491  \n",
       "2023-01-12                        0.797815                         0.042389  \n",
       "...                                    ...                              ...  \n",
       "2022-09-08                        0.879863                         0.090342  \n",
       "2020-09-09                        0.710299                         0.212441  \n",
       "2021-09-09                        0.586420                         0.268363  \n",
       "2022-09-09                        0.012185                         0.074712  \n",
       "2023-09-09                        0.197699                         0.270358  \n",
       "\n",
       "[862 rows x 7 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>returns</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>mean_pos_sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>neg_sentiment_lag1</th>\n",
       "      <th>neg_sentiment_diff</th>\n",
       "      <th>4 WEEKS BANK DISCOUNT</th>\n",
       "      <th>4 WEEKS COUPON EQUIVALENT</th>\n",
       "      <th>8 WEEKS BANK DISCOUNT</th>\n",
       "      <th>8 WEEKS COUPON EQUIVALENT</th>\n",
       "      <th>13 WEEKS BANK DISCOUNT</th>\n",
       "      <th>13 WEEKS COUPON EQUIVALENT</th>\n",
       "      <th>52 WEEKS BANK DISCOUNT</th>\n",
       "      <th>52 WEEKS COUPON EQUIVALENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>19.891333</td>\n",
       "      <td>20.328667</td>\n",
       "      <td>19.280001</td>\n",
       "      <td>19.978666</td>\n",
       "      <td>445813500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176692</td>\n",
       "      <td>0.162707</td>\n",
       "      <td>0.415098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950202</td>\n",
       "      <td>-0.699335</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>21.332666</td>\n",
       "      <td>21.620001</td>\n",
       "      <td>20.983334</td>\n",
       "      <td>21.081333</td>\n",
       "      <td>190264500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.035062</td>\n",
       "      <td>-0.035691</td>\n",
       "      <td>0.916847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250868</td>\n",
       "      <td>-0.235355</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>20.866667</td>\n",
       "      <td>21.252666</td>\n",
       "      <td>20.664667</td>\n",
       "      <td>21.000668</td>\n",
       "      <td>144627000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003826</td>\n",
       "      <td>-0.003834</td>\n",
       "      <td>0.643225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015513</td>\n",
       "      <td>0.065621</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-13</th>\n",
       "      <td>23.666668</td>\n",
       "      <td>23.755333</td>\n",
       "      <td>23.011999</td>\n",
       "      <td>23.073999</td>\n",
       "      <td>126301500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.010916</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>0.740729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081134</td>\n",
       "      <td>-0.028947</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>23.375999</td>\n",
       "      <td>23.520000</td>\n",
       "      <td>23.224001</td>\n",
       "      <td>23.478001</td>\n",
       "      <td>72135000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.008040</td>\n",
       "      <td>0.017608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052188</td>\n",
       "      <td>0.856669</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close     Volume  Dividends  \\\n",
       "2019-10-24  19.891333  20.328667  19.280001  19.978666  445813500        0.0   \n",
       "2019-10-29  21.332666  21.620001  20.983334  21.081333  190264500        0.0   \n",
       "2019-10-30  20.866667  21.252666  20.664667  21.000668  144627000        0.0   \n",
       "2019-11-13  23.666668  23.755333  23.011999  23.073999  126301500        0.0   \n",
       "2019-11-15  23.375999  23.520000  23.224001  23.478001   72135000        0.0   \n",
       "\n",
       "            Stock Splits   returns  log_returns  mean_pos_sentiment  ...  \\\n",
       "2019-10-24           0.0  0.176692     0.162707            0.415098  ...   \n",
       "2019-10-29           0.0 -0.035062    -0.035691            0.916847  ...   \n",
       "2019-10-30           0.0 -0.003826    -0.003834            0.643225  ...   \n",
       "2019-11-13           0.0 -0.010916    -0.010977            0.740729  ...   \n",
       "2019-11-15           0.0  0.008072     0.008040            0.017608  ...   \n",
       "\n",
       "            neg_sentiment_lag1  neg_sentiment_diff  4 WEEKS BANK DISCOUNT  \\\n",
       "2019-10-24            0.950202           -0.699335                   1.72   \n",
       "2019-10-29            0.250868           -0.235355                   1.63   \n",
       "2019-10-30            0.015513            0.065621                   1.59   \n",
       "2019-11-13            0.081134           -0.028947                   1.53   \n",
       "2019-11-15            0.052188            0.856669                   1.56   \n",
       "\n",
       "            4 WEEKS COUPON EQUIVALENT  8 WEEKS BANK DISCOUNT  \\\n",
       "2019-10-24                       1.75                   1.70   \n",
       "2019-10-29                       1.66                   1.64   \n",
       "2019-10-30                       1.62                   1.57   \n",
       "2019-11-13                       1.56                   1.54   \n",
       "2019-11-15                       1.59                   1.53   \n",
       "\n",
       "            8 WEEKS COUPON EQUIVALENT  13 WEEKS BANK DISCOUNT  \\\n",
       "2019-10-24                       1.73                    1.64   \n",
       "2019-10-29                       1.67                    1.60   \n",
       "2019-10-30                       1.60                    1.59   \n",
       "2019-11-13                       1.57                    1.54   \n",
       "2019-11-15                       1.56                    1.54   \n",
       "\n",
       "            13 WEEKS COUPON EQUIVALENT  52 WEEKS BANK DISCOUNT  \\\n",
       "2019-10-24                        1.67                    1.55   \n",
       "2019-10-29                        1.63                    1.55   \n",
       "2019-10-30                        1.62                    1.55   \n",
       "2019-11-13                        1.57                    1.53   \n",
       "2019-11-15                        1.57                    1.50   \n",
       "\n",
       "            52 WEEKS COUPON EQUIVALENT  \n",
       "2019-10-24                        1.59  \n",
       "2019-10-29                        1.59  \n",
       "2019-10-30                        1.59  \n",
       "2019-11-13                        1.57  \n",
       "2019-11-15                        1.54  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all into one dataframe\n",
    "#vix.index = pd.DatetimeIndex(vix.index.tz_localize(None))\n",
    "data_with_sentiment = prices_df.join(sentiment_df, how='inner')#.join(vix[['VIX Close']], how='left')\n",
    "data_with_sentiment = data_with_sentiment.drop(columns=['Date', 'adjusted_date'])\n",
    "\n",
    "data_with_sentiment['neg_sentiment_lag1'] = data_with_sentiment['mean_neg_sentiment'].shift(1)\n",
    "data_with_sentiment['neg_sentiment_diff'] = data_with_sentiment['mean_neg_sentiment'] - data_with_sentiment['neg_sentiment_lag1']\n",
    "data_with_sentiment = data_with_sentiment.dropna()\n",
    "\n",
    "data_with_sentiment = data_with_sentiment.join(t_rates_df)\n",
    "# Fill missing i/r data. Missing data is sparse, but might have to investigate.\n",
    "data_with_sentiment = data_with_sentiment.ffill()\n",
    "\n",
    "log_returns = data_with_sentiment['log_returns']\n",
    "\n",
    "data_with_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit a baseline GARCH(1,1) model without the exogenous term. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimising...\n",
      "Optimising finished in 1.144s\n",
      "  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: 3.7766319283422862\n",
      "        x: [ 6.358e-01 -2.818e+00 -1.928e-01]\n",
      "      nit: 53\n",
      "      jac: [-1.243e-06  4.441e-08 -6.262e-06]\n",
      "     nfev: 460\n",
      "     njev: 115\n",
      " hess_inv: <3x3 LbfgsInvHessProduct with dtype=float64>\n",
      "{'omega': 1.8885643766986113, 'alpha': 0.059734202639481274, 'beta': 0.8246689621981558}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>omega</th>\n",
       "      <td>1.888564</td>\n",
       "      <td>0.631978</td>\n",
       "      <td>2.988340</td>\n",
       "      <td>1.454178e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>0.059734</td>\n",
       "      <td>0.018171</td>\n",
       "      <td>3.287306</td>\n",
       "      <td>5.323667e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta</th>\n",
       "      <td>0.824669</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>17.455376</td>\n",
       "      <td>9.376498e-57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           coef   std err          t         P>|t|\n",
       "omega  1.888564  0.631978   2.988340  1.454178e-03\n",
       "alpha  0.059734  0.018171   3.287306  5.323667e-04\n",
       "beta   0.824669  0.047244  17.455376  9.376498e-57"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garch_baseline = GARCH(p=1, q=1, z=0, verbose=True)\n",
    "garch_baseline.train(100*log_returns)\n",
    "\n",
    "garch_baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -1875.246\n"
     ]
    }
   ],
   "source": [
    "baseline_log_like = garch_baseline.loglikelihood\n",
    "print(f\"Log likelihood: {baseline_log_like:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit model with the exogenous variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_sentiment = data_with_sentiment[['mean_pos_sentiment',\t'mean_neg_sentiment', 'mean_neutral_sentiment']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try alternative data using PCA since the three sentiments features are correlated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-learn\n",
    "#from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=1)\n",
    "# pca_exo_sentiment = pca.fit_transform(exo_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimising...\n",
      "Optimising finished in 1.774s\n",
      "  message: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  success: False\n",
      "   status: 2\n",
      "      fun: 4.123565025143794\n",
      "        x: [-5.667e-01 -7.423e-01 -6.464e-01 -5.887e-01 -5.457e-01\n",
      "            -6.772e-01]\n",
      "      nit: 4\n",
      "      jac: [-1.659e-01 -2.765e-01 -1.213e+00 -4.010e-02 -6.379e-02\n",
      "            -4.957e-03]\n",
      "     nfev: 658\n",
      "     njev: 94\n",
      " hess_inv: <6x6 LbfgsInvHessProduct with dtype=float64>\n",
      "{'omega': 0.5673913318159649, 'alpha': 0.47601397300750264, 'beta': 0.5239341056494212, 'gamma': array([[0.55505902, 0.57946488, 0.5080262 ]])}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>omega</th>\n",
       "      <td>0.567391</td>\n",
       "      <td>0.651334</td>\n",
       "      <td>0.871122</td>\n",
       "      <td>0.192001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>0.476014</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>89.441066</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta</th>\n",
       "      <td>0.523934</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>98.350422</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma[0]</th>\n",
       "      <td>0.555059</td>\n",
       "      <td>0.974950</td>\n",
       "      <td>0.569321</td>\n",
       "      <td>0.284666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma[1]</th>\n",
       "      <td>0.579465</td>\n",
       "      <td>0.869123</td>\n",
       "      <td>0.666724</td>\n",
       "      <td>0.252590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma[2]</th>\n",
       "      <td>0.508026</td>\n",
       "      <td>3.801052</td>\n",
       "      <td>0.133654</td>\n",
       "      <td>0.446858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              coef   std err          t     P>|t|\n",
       "omega     0.567391  0.651334   0.871122  0.192001\n",
       "alpha     0.476014  0.005322  89.441066  0.000000\n",
       "beta      0.523934  0.005327  98.350422  0.000000\n",
       "gamma[0]  0.555059  0.974950   0.569321  0.284666\n",
       "gamma[1]  0.579465  0.869123   0.666724  0.252590\n",
       "gamma[2]  0.508026  3.801052   0.133654  0.446858"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garch_with_sentiment = GARCH(p=1, q=1, z=1, verbose=True)\n",
    "garch_with_sentiment.train(100*log_returns, x=exo_sentiment)\n",
    "\n",
    "garch_with_sentiment.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -1991.122\n"
     ]
    }
   ],
   "source": [
    "with_sentiment_log_like = garch_with_sentiment.loglikelihood\n",
    "print(f\"Log likelihood: {with_sentiment_log_like:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fit with only negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>returns</th>\n",
       "      <th>log_returns</th>\n",
       "      <th>mean_pos_sentiment</th>\n",
       "      <th>...</th>\n",
       "      <th>neg_sentiment_lag1</th>\n",
       "      <th>neg_sentiment_diff</th>\n",
       "      <th>4 WEEKS BANK DISCOUNT</th>\n",
       "      <th>4 WEEKS COUPON EQUIVALENT</th>\n",
       "      <th>8 WEEKS BANK DISCOUNT</th>\n",
       "      <th>8 WEEKS COUPON EQUIVALENT</th>\n",
       "      <th>13 WEEKS BANK DISCOUNT</th>\n",
       "      <th>13 WEEKS COUPON EQUIVALENT</th>\n",
       "      <th>52 WEEKS BANK DISCOUNT</th>\n",
       "      <th>52 WEEKS COUPON EQUIVALENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-10-24</th>\n",
       "      <td>19.891333</td>\n",
       "      <td>20.328667</td>\n",
       "      <td>19.280001</td>\n",
       "      <td>19.978666</td>\n",
       "      <td>445813500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176692</td>\n",
       "      <td>0.162707</td>\n",
       "      <td>0.415098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.950202</td>\n",
       "      <td>-0.699335</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-29</th>\n",
       "      <td>21.332666</td>\n",
       "      <td>21.620001</td>\n",
       "      <td>20.983334</td>\n",
       "      <td>21.081333</td>\n",
       "      <td>190264500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.035062</td>\n",
       "      <td>-0.035691</td>\n",
       "      <td>0.916847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250868</td>\n",
       "      <td>-0.235355</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-30</th>\n",
       "      <td>20.866667</td>\n",
       "      <td>21.252666</td>\n",
       "      <td>20.664667</td>\n",
       "      <td>21.000668</td>\n",
       "      <td>144627000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003826</td>\n",
       "      <td>-0.003834</td>\n",
       "      <td>0.643225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015513</td>\n",
       "      <td>0.065621</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-13</th>\n",
       "      <td>23.666668</td>\n",
       "      <td>23.755333</td>\n",
       "      <td>23.011999</td>\n",
       "      <td>23.073999</td>\n",
       "      <td>126301500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.010916</td>\n",
       "      <td>-0.010977</td>\n",
       "      <td>0.740729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081134</td>\n",
       "      <td>-0.028947</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-15</th>\n",
       "      <td>23.375999</td>\n",
       "      <td>23.520000</td>\n",
       "      <td>23.224001</td>\n",
       "      <td>23.478001</td>\n",
       "      <td>72135000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008072</td>\n",
       "      <td>0.008040</td>\n",
       "      <td>0.017608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052188</td>\n",
       "      <td>0.856669</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-05</th>\n",
       "      <td>223.490005</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>222.250000</td>\n",
       "      <td>230.169998</td>\n",
       "      <td>119355000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049041</td>\n",
       "      <td>0.047876</td>\n",
       "      <td>0.377149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346941</td>\n",
       "      <td>-0.176921</td>\n",
       "      <td>5.08</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.04</td>\n",
       "      <td>5.15</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.07</td>\n",
       "      <td>4.59</td>\n",
       "      <td>4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-13</th>\n",
       "      <td>228.000000</td>\n",
       "      <td>232.669998</td>\n",
       "      <td>226.320007</td>\n",
       "      <td>230.289993</td>\n",
       "      <td>59515100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.266633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170020</td>\n",
       "      <td>0.323076</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.03</td>\n",
       "      <td>4.94</td>\n",
       "      <td>5.05</td>\n",
       "      <td>4.78</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.48</td>\n",
       "      <td>4.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-18</th>\n",
       "      <td>230.089996</td>\n",
       "      <td>235.679993</td>\n",
       "      <td>226.880005</td>\n",
       "      <td>227.199997</td>\n",
       "      <td>78010200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002940</td>\n",
       "      <td>-0.002945</td>\n",
       "      <td>0.949651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493096</td>\n",
       "      <td>-0.480494</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.79</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.77</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-23</th>\n",
       "      <td>242.610001</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>241.919998</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>86927200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049318</td>\n",
       "      <td>0.048140</td>\n",
       "      <td>0.489154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>0.306459</td>\n",
       "      <td>4.66</td>\n",
       "      <td>4.74</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.26</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-09-24</th>\n",
       "      <td>254.080002</td>\n",
       "      <td>257.190002</td>\n",
       "      <td>249.050003</td>\n",
       "      <td>254.270004</td>\n",
       "      <td>88491000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017080</td>\n",
       "      <td>0.016936</td>\n",
       "      <td>0.896809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319061</td>\n",
       "      <td>-0.309548</td>\n",
       "      <td>4.59</td>\n",
       "      <td>4.67</td>\n",
       "      <td>4.59</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.61</td>\n",
       "      <td>4.23</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>668 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close     Volume  \\\n",
       "2019-10-24   19.891333   20.328667   19.280001   19.978666  445813500   \n",
       "2019-10-29   21.332666   21.620001   20.983334   21.081333  190264500   \n",
       "2019-10-30   20.866667   21.252666   20.664667   21.000668  144627000   \n",
       "2019-11-13   23.666668   23.755333   23.011999   23.073999  126301500   \n",
       "2019-11-15   23.375999   23.520000   23.224001   23.478001   72135000   \n",
       "...                ...         ...         ...         ...        ...   \n",
       "2024-09-05  223.490005  235.000000  222.250000  230.169998  119355000   \n",
       "2024-09-13  228.000000  232.669998  226.320007  230.289993   59515100   \n",
       "2024-09-18  230.089996  235.679993  226.880005  227.199997   78010200   \n",
       "2024-09-23  242.610001  250.000000  241.919998  250.000000   86927200   \n",
       "2024-09-24  254.080002  257.190002  249.050003  254.270004   88491000   \n",
       "\n",
       "            Dividends  Stock Splits   returns  log_returns  \\\n",
       "2019-10-24        0.0           0.0  0.176692     0.162707   \n",
       "2019-10-29        0.0           0.0 -0.035062    -0.035691   \n",
       "2019-10-30        0.0           0.0 -0.003826    -0.003834   \n",
       "2019-11-13        0.0           0.0 -0.010916    -0.010977   \n",
       "2019-11-15        0.0           0.0  0.008072     0.008040   \n",
       "...               ...           ...       ...          ...   \n",
       "2024-09-05        0.0           0.0  0.049041     0.047876   \n",
       "2024-09-13        0.0           0.0  0.002089     0.002086   \n",
       "2024-09-18        0.0           0.0 -0.002940    -0.002945   \n",
       "2024-09-23        0.0           0.0  0.049318     0.048140   \n",
       "2024-09-24        0.0           0.0  0.017080     0.016936   \n",
       "\n",
       "            mean_pos_sentiment  ...  neg_sentiment_lag1  neg_sentiment_diff  \\\n",
       "2019-10-24            0.415098  ...            0.950202           -0.699335   \n",
       "2019-10-29            0.916847  ...            0.250868           -0.235355   \n",
       "2019-10-30            0.643225  ...            0.015513            0.065621   \n",
       "2019-11-13            0.740729  ...            0.081134           -0.028947   \n",
       "2019-11-15            0.017608  ...            0.052188            0.856669   \n",
       "...                        ...  ...                 ...                 ...   \n",
       "2024-09-05            0.377149  ...            0.346941           -0.176921   \n",
       "2024-09-13            0.266633  ...            0.170020            0.323076   \n",
       "2024-09-18            0.949651  ...            0.493096           -0.480494   \n",
       "2024-09-23            0.489154  ...            0.012602            0.306459   \n",
       "2024-09-24            0.896809  ...            0.319061           -0.309548   \n",
       "\n",
       "            4 WEEKS BANK DISCOUNT  4 WEEKS COUPON EQUIVALENT  \\\n",
       "2019-10-24                   1.72                       1.75   \n",
       "2019-10-29                   1.63                       1.66   \n",
       "2019-10-30                   1.59                       1.62   \n",
       "2019-11-13                   1.53                       1.56   \n",
       "2019-11-15                   1.56                       1.59   \n",
       "...                           ...                        ...   \n",
       "2024-09-05                   5.08                       5.17   \n",
       "2024-09-13                   4.94                       5.03   \n",
       "2024-09-18                   4.71                       4.79   \n",
       "2024-09-23                   4.66                       4.74   \n",
       "2024-09-24                   4.59                       4.67   \n",
       "\n",
       "            8 WEEKS BANK DISCOUNT  8 WEEKS COUPON EQUIVALENT  \\\n",
       "2019-10-24                   1.70                       1.73   \n",
       "2019-10-29                   1.64                       1.67   \n",
       "2019-10-30                   1.57                       1.60   \n",
       "2019-11-13                   1.54                       1.57   \n",
       "2019-11-15                   1.53                       1.56   \n",
       "...                           ...                        ...   \n",
       "2024-09-05                   5.04                       5.15   \n",
       "2024-09-13                   4.94                       5.05   \n",
       "2024-09-18                   4.70                       4.80   \n",
       "2024-09-23                   4.65                       4.75   \n",
       "2024-09-24                   4.59                       4.69   \n",
       "\n",
       "            13 WEEKS BANK DISCOUNT  13 WEEKS COUPON EQUIVALENT  \\\n",
       "2019-10-24                    1.64                        1.67   \n",
       "2019-10-29                    1.60                        1.63   \n",
       "2019-10-30                    1.59                        1.62   \n",
       "2019-11-13                    1.54                        1.57   \n",
       "2019-11-15                    1.54                        1.57   \n",
       "...                            ...                         ...   \n",
       "2024-09-05                    4.94                        5.07   \n",
       "2024-09-13                    4.78                        4.90   \n",
       "2024-09-18                    4.65                        4.77   \n",
       "2024-09-23                    4.53                        4.65   \n",
       "2024-09-24                    4.50                        4.61   \n",
       "\n",
       "            52 WEEKS BANK DISCOUNT  52 WEEKS COUPON EQUIVALENT  \n",
       "2019-10-24                    1.55                        1.59  \n",
       "2019-10-29                    1.55                        1.59  \n",
       "2019-10-30                    1.55                        1.59  \n",
       "2019-11-13                    1.53                        1.57  \n",
       "2019-11-15                    1.50                        1.54  \n",
       "...                            ...                         ...  \n",
       "2024-09-05                    4.59                        4.76  \n",
       "2024-09-13                    4.48                        4.65  \n",
       "2024-09-18                    4.37                        4.53  \n",
       "2024-09-23                    4.26                        4.41  \n",
       "2024-09-24                    4.23                        4.38  \n",
       "\n",
       "[668 rows x 25 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sentiment = data_with_sentiment[['mean_pos_sentiment']].to_numpy()#exo_sentiment[:, [1]]  # mean_pos_preamble_sentiment\n",
    "#neg_sentiment_normalised = (neg_sentiment - np.mean(neg_sentiment)) / np.var(neg_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Ljung-Box test (default lags=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = acorr_ljungbox(neg_sentiment, lags=[x for x in range(11)], return_df=True)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test exogenous data for stationarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF Test for X:\n",
      "ADF Statistic: -9.154977118460776\n",
      "p-value: 2.6246398261107458e-15\n",
      "Critical Values:\n",
      "   1%: -3.4402516575519346\n",
      "   5%: -2.8659091963995573\n",
      "   10%: -2.569096752341498\n",
      " The series is stationary (reject H0)\n"
     ]
    }
   ],
   "source": [
    "print(\"ADF Test for X:\")\n",
    "adf_test(neg_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when estimating the model parameters. \n",
      "Recommendation: pass in 100*y\n",
      "Optimising...\n",
      "Optimising finished in 1.140s\n",
      "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -5.415433348773559\n",
      "        x: [-6.912e+00 -2.345e+00 -1.645e+00 -7.246e+00]\n",
      "      nit: 31\n",
      "      jac: [ 3.739e-05  1.688e-06  1.430e-05  5.418e-06]\n",
      "     nfev: 205\n",
      "     njev: 41\n",
      " hess_inv: <4x4 LbfgsInvHessProduct with dtype=float64>\n",
      "{'omega': 0.00099626198175752, 'alpha': 0.09585820777749968, 'beta': 0.1929270926242445, 'gamma': array([[0.0007127]])}\n"
     ]
    }
   ],
   "source": [
    "garch_with_neg_sentiment = GARCH(p=1, q=1, z=1, verbose=True)\n",
    "garch_with_neg_sentiment.train(log_returns, x=neg_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>omega</th>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>3.575195</td>\n",
       "      <td>0.000188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>0.095858</td>\n",
       "      <td>0.033041</td>\n",
       "      <td>2.901204</td>\n",
       "      <td>0.001920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta</th>\n",
       "      <td>0.192927</td>\n",
       "      <td>0.196585</td>\n",
       "      <td>0.981391</td>\n",
       "      <td>0.163379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma[0]</th>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>1.865608</td>\n",
       "      <td>0.031268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              coef   std err         t     P>|t|\n",
       "omega     0.000996  0.000279  3.575195  0.000188\n",
       "alpha     0.095858  0.033041  2.901204  0.001920\n",
       "beta      0.192927  0.196585  0.981391  0.163379\n",
       "gamma[0]  0.000713  0.000382  1.865608  0.031268"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garch_with_neg_sentiment.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -1991.122\n"
     ]
    }
   ],
   "source": [
    "with_neg_sentiment_log_like = garch_with_sentiment.loglikelihood\n",
    "print(f\"Log likelihood: {with_neg_sentiment_log_like:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Fit with other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_data = data_with_sentiment[['4 WEEKS BANK DISCOUNT', 'mean_neg_sentiment']]   # '8 WEEKS BANK DISCOUNT',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimising...\n",
      "Optimising finished in 0.860s\n",
      "  message: ABNORMAL_TERMINATION_IN_LNSRCH\n",
      "  success: False\n",
      "   status: 2\n",
      "      fun: 4.092270075996755\n",
      "        x: [-7.016e-01 -7.819e-01 -6.180e-01 -7.082e-01 -6.326e-01]\n",
      "      nit: 2\n",
      "      jac: [-6.010e-02 -7.476e-02 -3.282e-01  1.556e-01 -1.671e-02]\n",
      "     nfev: 306\n",
      "     njev: 51\n",
      " hess_inv: <5x5 LbfgsInvHessProduct with dtype=float64>\n",
      "{'omega': 0.49577406929720225, 'alpha': 0.4575243769258762, 'beta': 0.538994845794549, 'gamma': array([[0.49253389, 0.53119737]])}\n"
     ]
    }
   ],
   "source": [
    "garch_with_neg_sentiment = GARCH(p=1, q=1, z=1, verbose=True)\n",
    "garch_with_neg_sentiment.train(100*log_returns, x=rf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junwei/Desktop/FYP Code/sentiment-analysis-volatility-forecasting-1/notebooks/Garch.py:469: RuntimeWarning: invalid value encountered in sqrt\n",
      "  diagnosis_df = pd.DataFrame(data={'coef': coef, 'std err': np.sqrt(info_mat_inv)}, index=index)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>t</th>\n",
       "      <th>P&gt;|t|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>omega</th>\n",
       "      <td>0.495774</td>\n",
       "      <td>0.146679</td>\n",
       "      <td>3.379987</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>0.457524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta</th>\n",
       "      <td>0.538995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma[0]</th>\n",
       "      <td>0.492534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gamma[1]</th>\n",
       "      <td>0.531197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              coef   std err         t     P>|t|\n",
       "omega     0.495774  0.146679  3.379987  0.000384\n",
       "alpha     0.457524       NaN       NaN       NaN\n",
       "beta      0.538995       NaN       NaN       NaN\n",
       "gamma[0]  0.492534       NaN       NaN       NaN\n",
       "gamma[1]  0.531197       NaN       NaN       NaN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garch_with_neg_sentiment.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood: -1980.669\n"
     ]
    }
   ],
   "source": [
    "with_neg_sentiment_log_like = garch_with_neg_sentiment.loglikelihood\n",
    "print(f\"Log likelihood: {with_neg_sentiment_log_like:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ll_null should be smaller than ll_alternative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlikelihood_ratio_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgarch_baseline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mgarch_with_neg_sentiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglikelihood\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m, in \u001b[0;36mlikelihood_ratio_test\u001b[0;34m(ll_null, ll_alternative, df)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDegrees of freedom must be positive.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ll_alternative \u001b[38;5;241m<\u001b[39m ll_null:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mll_null should be smaller than ll_alternative.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Compute test statistic\u001b[39;00m\n\u001b[1;32m      8\u001b[0m lr_stat \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (ll_alternative \u001b[38;5;241m-\u001b[39m ll_null)\n",
      "\u001b[0;31mValueError\u001b[0m: ll_null should be smaller than ll_alternative."
     ]
    }
   ],
   "source": [
    "likelihood_ratio_test(garch_baseline.loglikelihood, \n",
    "                      garch_with_neg_sentiment.loglikelihood, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE baseline: 509949.092\n",
      "MSE with sentiment: 511752.741\n"
     ]
    }
   ],
   "source": [
    "def mse(actual, pred):\n",
    "    return np.sum((actual-pred) ** 2)\n",
    "\n",
    "mse_baseline = mse(data_with_sentiment['VIX Close'], garch_baseline.sigma2)\n",
    "mse_sentiment = mse(data_with_sentiment['VIX Close'], garch_with_sentiment.sigma2)\n",
    "\n",
    "print(f\"MSE baseline: {mse_baseline:.3f}\")\n",
    "print(f\"MSE with sentiment: {mse_sentiment:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking with arch library to make sure we are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arch import arch_model\n",
    "\n",
    "model = arch_model(100*log_returns, vol='GARCH', mean='ARX', p=1, q=1)\n",
    "garch_fit = model.fit(disp='off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>AR - GARCH Model Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>log_returns</td>    <th>  R-squared:         </th>  <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Mean Model:</th>            <td>AR</td>         <th>  Adj. R-squared:    </th>  <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Vol Model:</th>            <td>GARCH</td>       <th>  Log-Likelihood:    </th> <td>  -1881.70</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Distribution:</th>        <td>Normal</td>       <th>  AIC:               </th> <td>   3771.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>        <td>Maximum Likelihood</td> <th>  BIC:               </th> <td>   3789.42</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th></th>                        <td></td>          <th>  No. Observations:  </th>     <td>668</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>           <td>Thu, Mar 13 2025</td>  <th>  Df Residuals:      </th>     <td>667</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>               <td>07:23:01</td>      <th>  Df Model:          </th>      <td>1</td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Mean Model</caption>\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>       <th>P>|t|</th>   <th>95.0% Conf. Int.</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Const</th> <td>    0.1394</td> <td>    0.150</td> <td>    0.928</td> <td>    0.354</td> <td>[ -0.155,  0.434]</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<caption>Volatility Model</caption>\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>       <th>P>|t|</th>    <th>95.0% Conf. Int.</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>omega</th>    <td>    2.1555</td> <td>    1.493</td> <td>    1.444</td> <td>    0.149</td>  <td>[ -0.771,  5.082]</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>alpha[1]</th> <td>    0.0662</td> <td>3.338e-02</td> <td>    1.984</td> <td>4.729e-02</td> <td>[7.918e-04,  0.132]</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>beta[1]</th>  <td>    0.8032</td> <td>    0.108</td> <td>    7.426</td> <td>1.120e-13</td>  <td>[  0.591,  1.015]</td> \n",
       "</tr>\n",
       "</table><br/><br/>Covariance estimator: robust"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:} &    log\\_returns    & \\textbf{  R-squared:         } &     0.000   \\\\\n",
       "\\textbf{Mean Model:}    &         AR         & \\textbf{  Adj. R-squared:    } &     0.000   \\\\\n",
       "\\textbf{Vol Model:}     &       GARCH        & \\textbf{  Log-Likelihood:    } &   -1881.70  \\\\\n",
       "\\textbf{Distribution:}  &       Normal       & \\textbf{  AIC:               } &    3771.41  \\\\\n",
       "\\textbf{Method:}        & Maximum Likelihood & \\textbf{  BIC:               } &    3789.42  \\\\\n",
       "\\textbf{}               &                    & \\textbf{  No. Observations:  } &    668      \\\\\n",
       "\\textbf{Date:}          &  Thu, Mar 13 2025  & \\textbf{  Df Residuals:      } &    667      \\\\\n",
       "\\textbf{Time:}          &      07:23:01      & \\textbf{  Df Model:          } &     1       \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{95.0\\% Conf. Int.}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Const} &       0.1394  &        0.150     &     0.928  &          0.354       &     [ -0.155,  0.434]       \\\\\n",
       "                  & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{95.0\\% Conf. Int.}  \\\\\n",
       "\\midrule\n",
       "\\textbf{omega}    &       2.1555  &        1.493     &     1.444  &          0.149       &     [ -0.771,  5.082]       \\\\\n",
       "\\textbf{alpha[1]} &       0.0662  &    3.338e-02     &     1.984  &      4.729e-02       &    [7.918e-04,  0.132]      \\\\\n",
       "\\textbf{beta[1]}  &       0.8032  &        0.108     &     7.426  &      1.120e-13       &     [  0.591,  1.015]       \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{AR - GARCH Model Results}\n",
       "\\end{center}\n",
       "\n",
       "Covariance estimator: robust"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           AR - GARCH Model Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:            log_returns   R-squared:                       0.000\n",
       "Mean Model:                        AR   Adj. R-squared:                  0.000\n",
       "Vol Model:                      GARCH   Log-Likelihood:               -1881.70\n",
       "Distribution:                  Normal   AIC:                           3771.41\n",
       "Method:            Maximum Likelihood   BIC:                           3789.42\n",
       "                                        No. Observations:                  668\n",
       "Date:                Thu, Mar 13 2025   Df Residuals:                      667\n",
       "Time:                        07:23:01   Df Model:                            1\n",
       "                               Mean Model                               \n",
       "========================================================================\n",
       "                 coef    std err          t      P>|t|  95.0% Conf. Int.\n",
       "------------------------------------------------------------------------\n",
       "Const          0.1394      0.150      0.928      0.354 [ -0.155,  0.434]\n",
       "                             Volatility Model                             \n",
       "==========================================================================\n",
       "                 coef    std err          t      P>|t|    95.0% Conf. Int.\n",
       "--------------------------------------------------------------------------\n",
       "omega          2.1555      1.493      1.444      0.149   [ -0.771,  5.082]\n",
       "alpha[1]       0.0662  3.338e-02      1.984  4.729e-02 [7.918e-04,  0.132]\n",
       "beta[1]        0.8032      0.108      7.426  1.120e-13   [  0.591,  1.015]\n",
       "==========================================================================\n",
       "\n",
       "Covariance estimator: robust\n",
       "\"\"\""
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garch_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Tesla Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_v1 = pd.read_csv('../data/tesla_gpt_summarised_v1.csv')\n",
    "tesla_v2 = pd.read_csv('../data/tesla_gpt_summarised_v2(strictprompt).csv')\n",
    "market_sentiment = pd.read_csv('../data/nyt_snp_headlines_with_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_v1['News'] = 'tesla'\n",
    "tesla_v2['News'] = 'tesla'\n",
    "market_sentiment['News'] = 'Market'\n",
    "relevant_columns = ['pos_sentiment','neg_sentiment','neutral_sentiment','pos_sentiment_w_preamb','neg_sentiment_w_preamb,neutral_sentiment_w_preamb','News','adjusted_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_v1['adjusted_date'] = pd.to_datetime(tesla_v1['adjusted_date'], format=\"%d/%m/%y\").dt.strftime(\"%Y-%m-%d\")\n",
    "tesla_v2['adjusted_date'] = pd.to_datetime(tesla_v2['adjusted_date'], format=\"%d/%m/%y\").dt.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['adjusted_date','pos_sentiment','neg_sentiment','neutral_sentiment','pos_sentiment_w_preamb','neg_sentiment_w_preamb','neutral_sentiment_w_preamb','News']\n",
    "tesla_v1_with_market = pd.concat([tesla_v1, market_sentiment],ignore_index=True)\n",
    "tesla_v2_with_market = pd.concat([tesla_v2, market_sentiment],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_v1_with_market = tesla_v1_with_market[cols]\n",
    "tesla_v2_with_market = tesla_v2_with_market[cols]\n",
    "tesla_v1_with_market.index = pd.DatetimeIndex(tesla_v1_with_market['adjusted_date'])\n",
    "tesla_v2_with_market.index = pd.DatetimeIndex(tesla_v2_with_market['adjusted_date'])\n",
    "tesla_v1_with_market = tesla_v1_with_market.drop(columns = 'adjusted_date')\n",
    "tesla_v2_with_market = tesla_v2_with_market.drop(columns = 'adjusted_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cleanup(prices_df, union_sentiment_df, option, stock_name):\n",
    "    \"\"\"\n",
    "    Cleans and merges stock price data with sentiment data based on the specified option.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prices_df : pandas.DataFrame\n",
    "        DataFrame containing stock price data with a datetime index.\n",
    "    union_sentiment_df : pandas.DataFrame\n",
    "        DataFrame containing sentiment data with a datetime index.\n",
    "    option : {1, 2, 3, 4}\n",
    "        An integer specifying the merging strategy:\n",
    "        - 1: Drop days without stock sentiment.\n",
    "        - 2: Use stock sentiment when available; otherwise, replace it with market sentiment.\n",
    "        - 3: Compute a 50-50 weighted aggregate for days with both stock and market sentiment.\n",
    "        - 4: Aggregate based on the number of news articles when both stock and market sentiment are present.\n",
    "    stock_name : str\n",
    "        The stock name (e.g., \"Tesla\", \"Microsoft\") as found in the \"News\" column of `union_sentiment_df`. \n",
    "        This is used to filter stock-specific sentiment data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A merged DataFrame with sentiment data adjusted according to the selected option.\n",
    "    \"\"\"\n",
    "    stocksentiment_df = union_sentiment_df[union_sentiment_df['News'] == stock_name] \n",
    "    marketsentiment_df = union_sentiment_df[union_sentiment_df['News'] == 'Market']\n",
    "\n",
    "    #Option 1: Drop the days without the stocks_df\n",
    "    if option == 1:\n",
    "        data_with_sentiment = prices_df.join(stocksentiment_df, how='inner')#.join(vix[['VIX Close']], how='left')\n",
    "        data_with_sentiment = data_with_sentiment.drop(columns=['Date'])\n",
    "        data_with_sentiment['neg_sentiment_lag1'] = data_with_sentiment['neg_sentiment'].shift(1)\n",
    "        data_with_sentiment['neg_sentiment_diff'] = data_with_sentiment['neg_sentiment'] - data_with_sentiment['neg_sentiment_lag1']\n",
    "        data_with_sentiment = data_with_sentiment.dropna()\n",
    "        data_with_sentiment = data_with_sentiment.join(t_rates_df)\n",
    "        # Fill missing i/r data. Missing data is sparse, but might have to investigate.\n",
    "        data_with_sentiment = data_with_sentiment.ffill()\n",
    "        log_returns = data_with_sentiment['log_returns']\n",
    "        data_with_sentiment = data_with_sentiment.reset_index()\n",
    "        relevant_columns = ['Date','Stock Splits','log_returns','pos_sentiment','neg_sentiment','neutral_sentiment', 'pos_sentiment_w_preamb','neg_sentiment_w_preamb','neutral_sentiment_w_preamb','neg_sentiment_lag1','neg_sentiment_diff','4 WEEKS BANK DISCOUNT']\n",
    "        data_with_sentiment = data_with_sentiment[relevant_columns]\n",
    "        agg_func = {\n",
    "            'pos_sentiment': 'mean',\n",
    "            'neg_sentiment': 'mean',\n",
    "            'neutral_sentiment': 'mean',\n",
    "            'pos_sentiment_w_preamb': 'mean',\n",
    "            'neg_sentiment_w_preamb': 'mean',\n",
    "            'neutral_sentiment_w_preamb': 'mean',\n",
    "            'Stock Splits': 'mean',\n",
    "            'log_returns': 'mean',\n",
    "            'neg_sentiment_lag1': 'mean',\n",
    "            'neg_sentiment_diff': 'mean',\n",
    "            '4 WEEKS BANK DISCOUNT': 'mean'}\n",
    "        column_rename = {\n",
    "            'pos_sentiment': 'mean_pos_sentiment',\n",
    "            'neg_sentiment': 'mean_neg_sentiment',\n",
    "            'neutral_sentiment': 'mean_neutral_sentiment',\n",
    "            'pos_sentiment_w_preamb': 'mean_pos_preamble_sentiment',\n",
    "            'neg_sentiment_w_preamb': 'mean_neg_preamble_sentiment',\n",
    "            'neutral_sentiment_w_preamb': 'mean_neutral_preamble_sentiment'}\n",
    "        final_df = data_with_sentiment.groupby(by='Date').agg(agg_func).rename(columns=column_rename).reset_index()\n",
    "        return final_df\n",
    "\n",
    "        \n",
    "    \n",
    "    #Option 2: \n",
    "    # On days where there is no stock sentiment, we replace it with market sentiment. \n",
    "    # On days with, we just use stock sentiment alone\n",
    "    if option == 2:\n",
    "        agg_func = {\n",
    "            'pos_sentiment': 'mean',\n",
    "            'neg_sentiment': 'mean',\n",
    "            'neutral_sentiment': 'mean',\n",
    "            'pos_sentiment_w_preamb': 'mean',\n",
    "            'neg_sentiment_w_preamb': 'mean',\n",
    "            'neutral_sentiment_w_preamb': 'mean'}\n",
    "        column_rename = {\n",
    "            'pos_sentiment': 'mean_pos_sentiment',\n",
    "            'neg_sentiment': 'mean_neg_sentiment',\n",
    "            'neutral_sentiment': 'mean_neutral_sentiment',\n",
    "            'pos_sentiment_w_preamb': 'mean_pos_preamble_sentiment',\n",
    "            'neg_sentiment_w_preamb': 'mean_neg_preamble_sentiment',\n",
    "            'neutral_sentiment_w_preamb': 'mean_neutral_preamble_sentiment'}\n",
    "        #stocksentiment_df = stocksentiment_df.drop(columns=['adjusted_date'])\n",
    "        #marketsentiment_df = marketsentiment_df.drop(columns=['adjusted_date'])\n",
    "        stocksentiment_df_agg = stocksentiment_df.groupby(by='adjusted_date').agg(agg_func).rename(columns=column_rename).reset_index()\n",
    "        stocksentiment_df_agg['News'] = stock_name\n",
    "        marketsentiment_df_agg = marketsentiment_df.groupby(by='adjusted_date').agg(agg_func).rename(columns=column_rename).reset_index()\n",
    "        marketsentiment_df_agg['News'] = 'Market'\n",
    "        stocksentiment_df_agg.index = pd.DatetimeIndex(stocksentiment_df_agg['adjusted_date'])\n",
    "        marketsentiment_df_agg.index = pd.DatetimeIndex(marketsentiment_df_agg['adjusted_date'])\n",
    "        data_with_sentiment = prices_df.join(stocksentiment_df_agg, how='left')#.join(vix[['VIX Close']], how='left')\n",
    "        data_with_sentiment = data_with_sentiment.join(marketsentiment_df_agg, how='left',lsuffix= stock_name, rsuffix= 'market')#.join(vix[['VIX Close']], how='left')\n",
    "        #print(data_with_sentiment.columns)\n",
    "        data_with_sentiment.rename(columns={\n",
    "            'mean_pos_sentimenttesla': 'mean_pos_sentiment',\n",
    "            'mean_neg_sentimenttesla': 'mean_neg_sentiment',\n",
    "            'mean_neutral_sentimenttesla': 'mean_neutral_sentiment',\n",
    "            'mean_pos_preamble_sentimenttesla': 'mean_pos_preamble_sentiment',\n",
    "            'mean_neg_preamble_sentimenttesla': 'mean_neg_preamble_sentiment',\n",
    "            'mean_neutral_preamble_sentimenttesla': 'mean_neutral_preamble_sentiment',\n",
    "            'Newstesla':'News'}, inplace=True)\n",
    "        \n",
    "        sentiment_columns = [\"mean_pos_sentiment\", \"mean_neg_sentiment\", \"mean_neutral_sentiment\",\n",
    "                         \"mean_pos_preamble_sentiment\", \"mean_neg_preamble_sentiment\", \"mean_neutral_preamble_sentiment\",'News']\n",
    "        \n",
    "        stock_columns = [col for col in sentiment_columns]\n",
    "        market_columns = [col + \"market\" for col in sentiment_columns]\n",
    "        # Ensure stock_columns and market_columns are correctly aligned\n",
    "        for stock_col, market_col in zip(stock_columns, market_columns):\n",
    "            data_with_sentiment[stock_col] = data_with_sentiment[stock_col].where(\n",
    "            data_with_sentiment[stock_col].notna(),  # Keep stock sentiment if it's NOT NaN\n",
    "            data_with_sentiment[market_col]  # Otherwise, replace with market sentiment\n",
    "            )\n",
    "\n",
    "        #rename_columns = {col + stock_name : col for col in sentiment_columns}\n",
    "        #data_with_sentiment.rename(columns=rename_columns, inplace=True)\n",
    "        market_sentiment_cols = [col + \"market\" for col in sentiment_columns]\n",
    "        data_with_sentiment.drop(columns=market_sentiment_cols, inplace=True)\n",
    "        return data_with_sentiment\n",
    "\n",
    "    #Option 3: \n",
    "    # On days where there is no stock sentiment, we replace it with market sentiment.\n",
    "    # On days where there is stock sentiment, we will do the aggregation (50% stock sentiment, 50% market sentiment)\n",
    "    if option == 3:\n",
    "        agg_func = {\n",
    "            'pos_sentiment': 'mean',\n",
    "            'neg_sentiment': 'mean',\n",
    "            'neutral_sentiment': 'mean',\n",
    "            'pos_sentiment_w_preamb': 'mean',\n",
    "            'neg_sentiment_w_preamb': 'mean',\n",
    "            'neutral_sentiment_w_preamb': 'mean'}\n",
    "        column_rename = {\n",
    "            'pos_sentiment': 'mean_pos_sentiment',\n",
    "            'neg_sentiment': 'mean_neg_sentiment',\n",
    "            'neutral_sentiment': 'mean_neutral_sentiment',\n",
    "            'pos_sentiment_w_preamb': 'mean_pos_preamble_sentiment',\n",
    "            'neg_sentiment_w_preamb': 'mean_neg_preamble_sentiment',\n",
    "            'neutral_sentiment_w_preamb': 'mean_neutral_preamble_sentiment'}\n",
    "        stocksentiment_df_agg = stocksentiment_df.groupby(by='adjusted_date').agg(agg_func).rename(columns=column_rename).reset_index()\n",
    "        marketsentiment_df_agg = marketsentiment_df.groupby(by='adjusted_date').agg(agg_func).rename(columns=column_rename).reset_index()\n",
    "        #This will give me a df where there are two entries per day if there exists both market and stock sentiment\n",
    "        unionsentiment_df_agg =  pd.concat([stocksentiment_df_agg, marketsentiment_df_agg], ignore_index=True)\n",
    "        unionsentiment_df_agg.index = pd.DatetimeIndex(unionsentiment_df_agg['adjusted_date'])\n",
    "        data_with_sentiment = prices_df.join(unionsentiment_df_agg, how='inner')#.join(vix[['VIX Close']], how='left')\n",
    "        data_with_sentiment = data_with_sentiment.drop(columns=['Date', 'adjusted_date'])\n",
    "        data_with_sentiment['neg_sentiment_lag1'] = data_with_sentiment['mean_neg_sentiment'].shift(1)\n",
    "        data_with_sentiment['neg_sentiment_diff'] = data_with_sentiment['mean_neg_sentiment'] - data_with_sentiment['neg_sentiment_lag1']\n",
    "        data_with_sentiment = data_with_sentiment.dropna()\n",
    "        data_with_sentiment = data_with_sentiment.join(t_rates_df)\n",
    "        data_with_sentiment = data_with_sentiment.reset_index()\n",
    "        relevant_columns = ['Date','Stock Splits','log_returns','mean_pos_sentiment','mean_neg_sentiment','mean_neutral_sentiment', 'mean_pos_preamble_sentiment','mean_neg_preamble_sentiment','mean_neutral_preamble_sentiment','neg_sentiment_lag1','neg_sentiment_diff','4 WEEKS BANK DISCOUNT']\n",
    "        data_with_sentiment = data_with_sentiment[relevant_columns]\n",
    "        agg_func = {\n",
    "            'mean_pos_sentiment': 'mean',\n",
    "            'mean_neg_sentiment': 'mean',\n",
    "            'mean_neutral_sentiment': 'mean',\n",
    "            'mean_pos_preamble_sentiment': 'mean',\n",
    "            'mean_neg_preamble_sentiment': 'mean',\n",
    "            'mean_neutral_preamble_sentiment': 'mean',\n",
    "            'Stock Splits': 'mean',\n",
    "            'log_returns': 'mean',\n",
    "            'neg_sentiment_lag1': 'mean',\n",
    "            'neg_sentiment_diff': 'mean',\n",
    "            '4 WEEKS BANK DISCOUNT': 'mean'}\n",
    "\n",
    "        data_with_sentiment_agg = data_with_sentiment.groupby(by='Date').agg(agg_func).reset_index()\n",
    "        return data_with_sentiment_agg\n",
    "\n",
    "    #Option 4: \n",
    "    # On days where there is no stock sentiment, we replace it with market sentiment.\n",
    "    # On days where there is stock sentiment, we aggregate based on the number of news articles.\n",
    "    if option == 4:\n",
    "        total_sentiment =  pd.concat([stocksentiment_df.reset_index(), marketsentiment_df.reset_index()], ignore_index=True)\n",
    "        agg_func = {\n",
    "            'pos_sentiment': 'mean',\n",
    "            'neg_sentiment': 'mean',\n",
    "            'neutral_sentiment': 'mean',\n",
    "            'pos_sentiment_w_preamb': 'mean',\n",
    "            'neg_sentiment_w_preamb': 'mean',\n",
    "            'neutral_sentiment_w_preamb': 'mean'}\n",
    "        column_rename = {\n",
    "            'pos_sentiment': 'mean_pos_sentiment',\n",
    "            'neg_sentiment': 'mean_neg_sentiment',\n",
    "            'neutral_sentiment': 'mean_neutral_sentiment',\n",
    "            'pos_sentiment_w_preamb': 'mean_pos_preamble_sentiment',\n",
    "            'neg_sentiment_w_preamb': 'mean_neg_preamble_sentiment',\n",
    "            'neutral_sentiment_w_preamb': 'mean_neutral_preamble_sentiment'}\n",
    "\n",
    "        total_sentiment_agg = total_sentiment.groupby(by='adjusted_date').agg(agg_func).reset_index()\n",
    "        total_sentiment_agg.index = pd.DatetimeIndex(total_sentiment_agg['adjusted_date'])\n",
    "        data_with_sentiment = prices_df.join(total_sentiment_agg, how='left')#.join(vix[['VIX Close']], how='left')\n",
    "        return data_with_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_v1_1 = merge_cleanup(prices_df, tesla_v1_with_market,1,'tesla')\n",
    "tesla_v1_2 = merge_cleanup(prices_df, tesla_v1_with_market,2,'tesla')\n",
    "tesla_v1_3 = merge_cleanup(prices_df, tesla_v1_with_market,3,'tesla')\n",
    "tesla_v1_4 = merge_cleanup(prices_df, tesla_v1_with_market,4,'tesla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_v2_1 = merge_cleanup(prices_df, tesla_v2_with_market,1,'tesla')\n",
    "tesla_v2_2 = merge_cleanup(prices_df, tesla_v2_with_market,2,'tesla')\n",
    "tesla_v2_3 = merge_cleanup(prices_df, tesla_v2_with_market,3,'tesla')\n",
    "tesla_v2_4 = merge_cleanup(prices_df, tesla_v2_with_market,4,'tesla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garch_with_sentiment = GARCH(p=1, q=1, z=1, verbose=True)\n",
    "garch_with_sentiment.train(100*log_returns, x=exo_sentiment)\n",
    "\n",
    "garch_with_sentiment.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
