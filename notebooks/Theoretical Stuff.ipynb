{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define the GARCHX model\n",
    "\n",
    "Mean Model\n",
    "$$ r_t = \\mu + \\sigma_t z_t $$ \n",
    "$$z_t \\sim i.i.d\\ N(0, 1)$$\n",
    "where $e_t = \\sigma_t z_t$.\n",
    "\n",
    "Volatility Model \n",
    "$$ \\sigma_t^2 = \\omega +  \\alpha e_{t-1}^2 + \\beta \\sigma_{t-1}^2 + \n",
    "    \\gamma x_t^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Log likelihood\n",
    "For simplicity, assume $\\mu=0$. Let $\\theta = (\\omega, \\alpha, \\beta, \\gamma)$ . Then the log likelihood function is defined as a function of the conditional densities of $r_t$ as such: \n",
    " $$ l(\\theta) =  \\sum_{t=1}^T {\\frac{1}{2} ({-\\log{2\\pi} -\\log{\\sigma_t^2} - \\frac{e_t^2}{\\sigma_t^2}})} $$\n",
    "\n",
    "The first and second partial derivatives of $l(\\theta)$ are as follows:\n",
    "$$ \\frac{\\partial }{\\partial \\theta} l(\\theta) =  \\sum_{t=1}^T {\\frac{1}{2} ({\\frac{e_t^2}{\\sigma_t^4} - \\frac{1}{\\sigma_t^2}})} \\frac{\\partial \\sigma_t^2}{\\partial \\theta}$$\n",
    "\n",
    "$$ \\begin{split}\n",
    "\\frac{\\partial^2 }{\\partial \\theta_1 \\partial \\theta_2} l(\\theta) &= - \\frac{1}{2} \\sum_{t=1}^T\\left( \\frac{\\partial^2 \\sigma_t^2}{\\partial \\theta_1 \\partial \\theta_2} (\\frac{1}{\\sigma_t^2} - \\frac{e_t^2}{\\sigma_t^4}) + \n",
    "\\frac{\\partial \\sigma_t^2}{\\partial \\theta_1} \\frac{\\partial \\sigma_t^2}{\\partial \\theta_2} (\\frac{2e_t^2}{\\sigma_t^6} - \\frac{1}{\\sigma_t^4})\\right) \n",
    "\n",
    "\\end{split} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Partial Derivatives of $\\sigma^2$\n",
    "\n",
    "This section supplements the previous section, as the first and second partial derivatives of $\\sigma^2$ are used in the score funtion and information matrix.\n",
    "\n",
    "#### 3.1 First Partial Derivative\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\omega} &= 1 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\omega} \n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\alpha} &= e_{t-1}^2 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\alpha}\n",
    "\\end{split} $$\n",
    "\n",
    " \n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\beta} &= \\sigma_{t-1}^2 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\beta}\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\gamma} &= x_{t-1}^2 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\gamma}\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "Unconditional Expectation of first derivatives of $\\sigma^2$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial \\sigma_{t}^2}{\\partial \\omega}\\right] = \\frac{1}{1-\\beta} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial \\sigma_{t}^2}{\\partial \\alpha}\\right] = \\frac{\\omega + \\gamma \\mu_{x^2}}{(1-\\beta)(1-\\alpha-\\beta)} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial \\sigma_{t}^2}{\\partial \\beta}\\right] = \\frac{\\omega + \\gamma \\mu_{x^2}}{(1-\\beta)(1-\\alpha-\\beta)} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial \\sigma_{t}^2}{\\partial \\gamma}\\right] = \\frac{\\mu_{x^2}}{1-\\beta} $$\n",
    "\n",
    "\n",
    "#### 3.2. Second Derivative to $\\sigma^2$\n",
    "There exist two cases. \n",
    "\n",
    "a). If both parameters $ \\theta_i, \\theta_j $ are not $\\beta$,\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial^2 \\sigma_{t}^2}{\\partial \\theta_i \\theta_j} &= \\beta \\frac{\\partial^2 \\sigma_{t-1}^2}{\\partial \\theta_i \\theta_j} \n",
    "\\end{split} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\theta_i \\partial\\theta_j}\\right] = 0 $$\n",
    "\n",
    "\n",
    "b). If at least one of the parameters $\\theta_i, \\theta_j$ is $\\beta$,\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial^2 \\sigma_{t}^2}{\\partial \\beta \\theta_j} &= \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\theta_j } + \\beta \\frac{\\partial^2 \\sigma_{t-1}^2}{\\partial \\beta \\theta_j} \n",
    "\\end{split} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\beta\\partial\\omega}\\right] = \\frac{1}{(1-\\beta)^2}$$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\beta\\partial\\alpha}\\right] = \\frac{\\omega + \\gamma \\mu_{x^2}}{(1-\\beta)^2(1-\\alpha-\\beta)} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\beta^2}\\right] = \\frac{\\omega + \\gamma \\mu_{x^2}}{(1-\\beta)^2(1-\\alpha-\\beta)} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\beta\\partial\\gamma}\\right] = \\frac{1}{(1-\\beta)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Stationarity of time series.\n",
    "#### 4.1 Stationarity of $\\sigma_t^2$\n",
    "We know that $e_t$ = $\\sigma_t$ $z_t$, \\\n",
    "$since \\ z_t \\ is \\sim i.i.d \\ N(0,1)$\n",
    "$$ \\begin{split}\n",
    "E[e_t^2] &= E[\\sigma_t^2 z_t^2]\\\\\n",
    "&= E[\\sigma_t^2] E[z_t^2] \\\\\n",
    "&= E[\\sigma_t^2]\n",
    "\\\\\n",
    "\\end{split} $$\n",
    "\n",
    "*Note that $E[z_t^2]$ is a expectation of a chi-squared distribution of degree of freedom 1*\n",
    "\n",
    "We use the fact that $E[e_t^2]$ = $E[\\sigma_t^2]$\n",
    "\n",
    "$$ \\begin{split}\n",
    "    E[\\sigma_t^2] &= E[\\omega + \\alpha \\sigma_{t-1}^2 + \\beta e_{t-1}^2 + \\gamma x_t^2] \\\\\n",
    "    &= E[\\omega] + \\alpha E[\\sigma_{t-1}^2] + \\beta E[e_{t-1}^2] + \\gamma E[x_t^2] \\\\\n",
    "    &= \\omega + \\gamma E[x_t^2] + \\alpha E[\\sigma_{t-1}^2] + \\beta E[e_{t-1}^2] \\\\\n",
    "    &= \\omega + \\gamma k + (\\alpha + \\beta) E[\\sigma_{t-1}^2] \\\\\n",
    "    &= \\omega + \\gamma k + (\\alpha + \\beta) E[\\omega + \\alpha \\sigma_{t-2}^2 + \\beta e_{t-2}^2 + \\gamma x_{t-1}^2] \\\\\n",
    "    &= \\ldots \\\\\n",
    "    &= (\\omega + \\gamma k) + [1+ (\\alpha + \\beta) + (\\alpha + \\beta)^2 + (\\alpha + \\beta)^3 + \\ldots] \\\\\n",
    "    &= \\frac{\\omega + \\gamma k}{1-(\\alpha+\\beta)}\n",
    "\\end{split} $$\n",
    "where k = $E[x_t^2]$ which is a constant, given that we assume $x_t^2$ is stationary \n",
    "\n",
    "From this, we know that $|\\alpha + \\beta|$ < 1.\n",
    "\n",
    "\\\n",
    "Since, $x_t^2$ is stationary, then $E[x_t^2, x_{t-s}^2]$ is not dependent on t\n",
    "\n",
    "Following that $x_t^2$ is stationary,\n",
    "\n",
    "We prove that\n",
    "$$ \\begin{split}\n",
    "Cov[\\sigma_t^2,\\sigma_{t-s}^2] &= E[\\sigma_t^2 \\sigma_{t-s}^2] - E[\\sigma_t^2]E[\\sigma_{t-s}^2] \\\\\n",
    "&= E[\\sigma_t^2 \\sigma_{t-s}^2] - (\\frac{\\omega + \\gamma k}{1-(\\alpha+\\beta)}) ^2\\\\\n",
    "\\\\\n",
    "\\end{split} $$\n",
    "\n",
    "We found \n",
    "$$ \\begin{split}\n",
    "E[\\sigma_t^2 \\sigma_{t-s}^2] &= E[(\\omega + \\alpha \\sigma_{t-1}^2 + \\beta e_{t-1}^2 + \\gamma x_t^2)(\\omega + \\alpha \\sigma_{t-2}^2 + \\beta e_{t-2}^2 + \\gamma x_{t-1}^2)] \\\\\n",
    "&= \\ldots \\\\\n",
    "&= (\\omega^2 + 2\\gamma \\omega k) + (2 \\alpha \\omega + 2 \\beta \\omega + 2 \\alpha \\gamma k + 2 \\beta \\gamma k)(\\frac{\\omega + \\gamma k}{1-(\\alpha+\\beta)}) + (\\alpha + \\beta) ^ 2 E[\\sigma_{t-1}^2 \\sigma_{t-s-1}^2] + \\gamma^2 E[x_t^2 x_{t-s}^2]\n",
    "\n",
    "\\end{split} $$\n",
    "\n",
    "Since we know $x_t$ is stationary, \n",
    "$Cov[x_t^2,x_{t-s}^2] = E[x_t^2 x_{t-s}^2] - E[x_t^2]E[x_{t-s}^2]$ does not depend on t, \n",
    "\n",
    "Thus we can conclude that $E[x_t^2 x_{t-s}^2]$ does not depend on t \n",
    "\n",
    "This shows that $Cov[\\sigma_t^2,\\sigma_{t-s}^2]$ does not depend on t.\n",
    "\n",
    "\n",
    "\n",
    "We see that the following is a stationary AR(1) process now as $(\\alpha+\\beta) < 1 \\implies  (\\alpha+\\beta)^2 < 1$:\n",
    "\n",
    "$$\\begin{split} &E[\\sigma_t^2 \\sigma_{t-s}^2] = (w^2 + 2\\gamma \\omega k) + 2(\\alpha\\omega + \\beta\\omega + \\alpha\\gamma k + \\beta\\gamma k) \\frac{\\omega+\\gamma k}{1-(\\alpha+\\beta)} + (\\alpha+\\beta)^2 E(\\sigma_{t-1}^2\\sigma_{t-s-1}^2) + \\gamma^2 E(x_t^2 x_{t-s}^2) \\\\\n",
    "\n",
    "&\\implies E[\\sigma_t^2 \\sigma_{t-s}^2] \\text{ does not depend on }t \\\\\n",
    "&\\implies cov(\\sigma_t^2, \\sigma_{t-s}^2) = E[\\sigma_t^2 \\sigma_{t-s}^2] - \\left(\\frac{\\omega+\\gamma k}{1-(\\alpha+\\beta)}\\right)^2\\text{ does not depend on }t\\\\\n",
    "\n",
    "&\\therefore \\{\\sigma_t^2\\} \\text{ is stationary}.\n",
    "\n",
    "\\end{split}$$\n",
    "\n",
    "\n",
    "#### 4.2 Stationary of $r_t$\n",
    "\n",
    "First note that $z_t \\sim i.i.d \\ N(0,1) \\implies E[z_t] = 0$\n",
    "\n",
    "First, we show $E[r_t]$ is constant. \n",
    "$$\\begin{split}\n",
    "E[r_t] &= E[\\mu + \\sigma_t z_t] \\\\\n",
    "        &= \\mu + E[\\sigma_t z_t] \\\\\n",
    "        &= \\mu + E[\\sigma_t]E[z_t] \\\\\n",
    "        &= \\mu\n",
    "\\end{split} $$\n",
    "\n",
    "Next, we show finite variance of $r_t$:\n",
    "$$\\begin{split}\n",
    "Var(r_t) &= E[\\sigma_t^2] \\\\ \n",
    "        &= \\frac{\\omega + \\gamma k}{1-(\\alpha+\\beta)}\n",
    "\\end{split} $$\n",
    "\n",
    "Lastly, we show that the autocovariance of $r_t$ is not time dependent:\n",
    "$$\\begin{split}\n",
    "Cov(r_t, r_{t-r}) &= Cov(e_t, e_{t-r}) \\\\ \n",
    "        &= E[e_t e_{t-r}] -E[e_t] E[e_{t-r}] \\\\\n",
    "        &= 0\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 4.3 Convergence of e_t\n",
    "\n",
    "We know that $e_t$ = $\\sigma_t$ $z_t$\n",
    "\n",
    "and $z_t$ is\n",
    "$$z_t  \\sim i.i.d \\ N(0, 1)$$\n",
    "\n",
    "Therefore, \n",
    "$$ \\begin{split}\n",
    "    E[e_{t}| \\mathcal{F}_{t-1}] &= E[\\sigma_t z_t| \\mathcal{F}_{t-1}] \\\\\n",
    "    &= E[z_{t}| \\mathcal{F}_{t-1}] \\ E[\\sigma_t| \\mathcal{F}_{t-1}] \\\\\n",
    "    &= 0\n",
    "\\end{split} $$\n",
    "we conclude that $e_t$ is an MDS sequence\n",
    "\n",
    "For simplicity, we assume our time series is ergodic.\n",
    "\n",
    "\n",
    "\n",
    "By the Martingale CLT, as $e_t$ is stationary and ergodic Martingale difference, \n",
    "$$ \\frac{1}{\\sqrt{n}} (\\sum_{t=1}^n e_t) \\xrightarrow[]{\\text{d}} N(0, \\sigma^2) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5 Consistency of the MLE.\n",
    "\n",
    "Let the score function be defined as $s(\\theta) = \\frac{\\partial}{\\partial\\theta} l(\\theta)$ and the information $I(\\theta) = \\frac{\\partial^2}{\\partial\\theta^2} l(\\theta)$. \n",
    "\n",
    "Under suitable regularity conditions, and having shown that the time series is stationary and ergodic,\n",
    "\n",
    "$$ \\hat{\\theta} \\xrightarrow[]{\\text{p}} \\theta_0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Asymptotic normality of the MLE.\n",
    "\n",
    "First, we consider the distribution of the true score $s(\\theta_0)$. Under the regularity conditions, it can be shown that $E(s_n(\\theta_0)) = 0$  and $Var(s_n(\\theta_0)) = I_E(\\theta_0)$. Furthermore as we have shown that $r_t $ converges in distribution by the Martingale CLT, the score function\n",
    "\n",
    "$$\\begin{split}\n",
    " \\frac{1}{\\sqrt{n}} s_n(\\theta_0) &= \\sum_{t=1}^n \\frac{\\partial}{\\partial\\theta}\\log f(r_t | \\theta_0) \\\\ \n",
    " &\\xrightarrow[]{\\text{d}} N(0, I_E(\\theta_0))\n",
    " \n",
    "\\end{split}$$\n",
    "\n",
    "converges in distribution to a multivariate normal distribution $N(0, I_E(\\theta_0))$.\n",
    "\n",
    "\n",
    "Next, consider a first order multivariate Taylor expansion of the score function at $\\theta_0$ about $\\hat{\\theta}$:\n",
    "\n",
    "$$ s_n(\\theta_0) \\approx s_n(\\hat{\\theta}) - I_n(\\theta_0) (\\theta_0 - \\hat{\\theta}) $$\n",
    "\n",
    "Since by definition $s_n(\\hat{\\theta}) = 0$,\n",
    "$$\\begin{split}\n",
    " s_n(\\theta_0) &= - I_n(\\theta_0) (\\theta_0 - \\hat{\\theta}) \\\\\n",
    " (\\hat{\\theta} - \\theta_0) &=  I_n(\\theta_0)^{-1} s_n(\\theta_0) \\\\\n",
    " \\sqrt{n}(\\hat{\\theta} - \\theta_0) &=  \\sqrt{n} I_n(\\theta_0)^{-1} s_n(\\theta_0) \n",
    "\\end{split} $$\n",
    "\n",
    "As shown earlier,\n",
    "\n",
    "$$ \\frac{1}{\\sqrt{n}} s_n(\\theta_0)  \\xrightarrow[]{\\text{d}} N(0, I_E(\\theta_0)) $$ \n",
    "\n",
    "and by the law of large numbers,\n",
    "\n",
    "$$\\frac{1}{n} I_n(\\theta_0) = \\frac{1}{n} \\sum_{t=1}^n \\frac{\\partial^2}{\\partial\\theta^2} \\log(f(x_i|\\theta))  \\xrightarrow[]{\\text{p}} I_E(\\theta_0)$$ \n",
    "\n",
    "\n",
    "Therefore by Slutsky's theorem,\n",
    "\n",
    "$$ \\sqrt{n}(\\hat{\\theta} - \\theta_0) \\xrightarrow[]{\\text{d}} I_E(\\theta_0)^{-1} N(0, I(\\theta_0)) = N(0, I_E(\\theta_0)^{-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Asymptotic distribution of the MLE\n",
    "\n",
    "The fisher information is given by \n",
    "\n",
    "$$ I(\\theta) = - \\begin{pmatrix}\n",
    "\\frac{\\partial^2 l}{\\partial \\omega^2} & \\frac{\\partial^2 l}{\\partial \\omega \\partial \\alpha} & \\frac{\\partial^2 l}{\\partial \\omega \\partial \\beta} & \\frac{\\partial^2 l}{\\partial \\omega \\partial \\gamma} \\\\\n",
    "\n",
    "\\frac{\\partial^2 l}{\\partial \\alpha \\partial \\omega} & \\frac{\\partial^2 l}{\\partial \\alpha^2} & \\frac{\\partial^2 l}{\\partial \\alpha \\partial \\beta} & \\frac{\\partial^2 l}{\\partial \\alpha \\partial \\gamma} \\\\\n",
    "\n",
    "\\frac{\\partial^2 l}{\\partial \\beta \\partial \\omega} & \\frac{\\partial^2 l}{\\partial \\beta \\partial \\alpha} & \\frac{\\partial^2 l}{\\partial \\beta^2} & \\frac{\\partial^2 l}{\\partial \\beta \\partial \\gamma} \\\\\n",
    "\n",
    "\\frac{\\partial^2 l}{\\partial \\gamma \\partial \\omega} & \\frac{\\partial^2 l}{\\partial \\gamma \\partial \\alpha} & \\frac{\\partial^2 l}{\\partial \\gamma \\partial \\beta} & \\frac{\\partial^2 l}{\\partial \\gamma^2} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "where each partial derivative is given above.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Assuming consistency (we probably prove this before this part?) / If the regularity conditions hold, the asymptotic distribution of the MLE $\\hat{\\theta}$ converges to a multivariate normal distribution with expected value $\\theta$ and variance $I(\\theta)^{-1}$,i.e. $$\\hat{\\theta} \\sim MVN_d(\\theta_0, I(\\hat{\\theta})^{-1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Consistency of $\\sigma_t^2$ and $r_t^2$\n",
    "to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymtotic distribution of parameters.\n",
    "\n",
    "1. Find first partial derivative for each parameter. For example for $\\gamma$,\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial l}{\\partial \\gamma} &= \\frac{\\partial l}{\\partial \\sigma^2} \\frac{\\partial \\sigma^2}{\\partial \\gamma} \\\\\n",
    "    &= \\sum_{t=1}^T \\frac{x_t^2}{2\\sigma_t^2} (\\frac{e_{t}^2}{\\sigma_t^2} -1)\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "\n",
    "2.  Find second partial derivative for each parameter, to each parameter. For four parameters, we will have 16 partial derivatives. For instance,\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial^2 l}{\\partial \\gamma^2} &= \\frac{\\partial}{\\partial \\sigma^2} (\\frac{\\partial l }{\\partial \\gamma}) \\frac{\\partial \\sigma^2}{\\partial \\gamma} \\\\\n",
    "    &= \\sum_{t=1}^T \\frac{x_t^4}{\\sigma_t^4} (\\frac{1}{2} - \\frac{e_t^2}{\\sigma_t^2})\n",
    "\\end{split} $$\n",
    "\n",
    "3. Find the expectation of the negative of each second partial derivative. Question: do we need to find the expectation, or is the raw form of the second derivative (such as shown in step 2) sufficient?\n",
    "\n",
    "4. This 4x4 matrix then forms our fisher information matrix, $I_E(\\theta)$ where $\\theta$ is the vector of our parameters.\n",
    "\n",
    "5. The asymptotic distribution of our parameters follows a multi variate normal distribution with mean $(\\omega_0, \\alpha_0, \\beta_0, \\gamma_0), $ and variance as the inverse fisher information matrix mentioned in step 3. \n",
    "\n",
    "6. Using the variance, we can find the p value associated with each parameter.\n",
    "\n",
    "---\n",
    "Questions:\n",
    "1. Do we need the regularity conditions for the log likelihood function to hold in order for asymptotic normality and consistency to hold? If so, is this where our assumptions of parameter boundaries and distributions come into place?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for first derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "    Compute the partial derivative of l with respect to alpha.\n",
    "    \n",
    "    Parameters:\n",
    "    x : np.array\n",
    "        The input time series data.\n",
    "    sigma_squared : np.array\n",
    "        The variance values \\( \\sigma_t^2 \\) for each time step.\n",
    "    e : np.array\n",
    "        The error terms \\( e_t \\) for each time step.\n",
    "    \n",
    "    Returns:\n",
    "    float\n",
    "        The computed derivative value.\n",
    "    \"\"\"\n",
    "\n",
    "### a.\n",
    "def partial_l_gamma(x, sigma_squared, e):\n",
    "    T = len(x)\n",
    "    derivative = np.sum((x**2 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n",
    "\n",
    "### b.\n",
    "def partial_l_omega(sigma_squared, e):\n",
    "    T = len(x)\n",
    "    derivative = np.sum((1 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n",
    "\n",
    "### c.\n",
    "def partial_l_alpha(x, sigma_squared, e):\n",
    "    T = len(x)\n",
    "    derivative = np.sum((x**2 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n",
    "\n",
    "### d.\n",
    "def partial_l_beta(x, sigma_squared, e):\n",
    "    sigma_squared_tminus1 = sigma_squared.shift(1)\n",
    "    T = len(x)\n",
    "    derivative = np.sum((sigma_squared_tminus1 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proving consistency: <a href='https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Consistency'>Consistency</a>\n",
    "Regulartiy conditions: <a href='https://en.wikipedia.org/wiki/Fisher_information#Regularity_conditions'>Regularity, Fisher Information</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of consistency\n",
    "1. Ergodicity,\n",
    "2. Stationarity,\n",
    "3. $\\theta_0$ is not on the boundary of the parameter space.\n",
    "\n",
    "If a time series $X_t$ is stationary and erogdic, then \n",
    "$$\\frac{1}{T} \\sum_{t=1}^T X_t \\xrightarrow[]{\\text{p}} \\mu $$ \n",
    "where  $\\mu = E[X_t] < \\infty$.\n",
    "\n",
    "### Ergodic Theorem\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Notes\n",
    "$x_t$ stationary and ergodic => $x_t^2$ stationary and ergodic\n",
    "\n",
    "\n",
    "### Backlog:\n",
    "- Testing raw data for stationary and ergodicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-125.72244114788363"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "s0 = 90\n",
    "u = 1.2\n",
    "d = 0.8\n",
    "r = 0.1\n",
    "K = 100\n",
    "\n",
    "s0 * (d - u*np.e**(-1*r)) - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7036.015762647621"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "90**2 * 1.2 * 0.8 * np.e**(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.2 * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
