{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Define the GARCHX model\n",
    "\n",
    "Mean Model\n",
    "$$ r_t = \\mu + \\sigma_t z_t $$ \n",
    "$$z_t | \\mathcal{F}_{t-1} \\sim N(0, 1)$$\n",
    "where $e_t = \\sigma_t z_t$.\n",
    "\n",
    "Volatility Model \n",
    "$$ \\sigma_t^2 = \\omega +  \\alpha e_{t-1}^2 + \\beta \\sigma_{t-1}^2 + \n",
    "    \\gamma x_t^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Log likelihood\n",
    "For simplicity, assume $\\mu=0$. Let $\\theta = (\\omega, \\alpha, \\beta, \\gamma)$ . Then the log likelihood function is defined as a function of the conditional densities of $r_t$ as such: \n",
    " $$ l(\\theta) =  \\sum_{t=1}^T {\\frac{1}{2} ({-\\log{2\\pi} -\\log{\\sigma_t^2} - \\frac{e_t^2}{\\sigma_t^2}})} $$\n",
    "\n",
    "The first and second partial derivatives of $l(\\theta)$ are as follows:\n",
    "$$ \\frac{\\partial }{\\partial \\theta} l(\\theta) =  \\sum_{t=1}^T {\\frac{1}{2} ({\\frac{e_t^2}{\\sigma_t^4} - \\frac{1}{\\sigma_t^2}})} \\frac{\\partial \\sigma_t^2}{\\partial \\theta_2}$$\n",
    "\n",
    "$$ \\begin{split}\n",
    "\\frac{\\partial }{\\partial \\theta_1 \\partial \\theta_2} l(\\theta) &= - \\frac{1}{2} \\sum_{t=1}^T\\left( \\frac{\\partial^2 \\sigma_t^2}{\\partial \\theta_1 \\partial \\theta_2} (\\frac{1}{\\sigma_t^2} - \\frac{e_t^2}{\\sigma_t^4}) + \n",
    "\\frac{\\partial \\sigma_t^2}{\\partial \\theta_1} \\frac{\\partial \\sigma_t^2}{\\partial \\theta_2} (\\frac{2e_t^2}{\\sigma_t^6} - \\frac{1}{\\sigma_t^4})\\right) \n",
    "\n",
    "\\end{split} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Partial Derivatives of $\\sigma^2$\n",
    "\n",
    "This section supplements the previous section, as the first and second partial derivatives of $\\sigma^2$ are used in the score funtion and information matrix.\n",
    "\n",
    "#### 3.1 First Partial Derivative\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\omega} &= 1 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\omega} \n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\alpha} &= e_{t-1}^2 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\alpha}\n",
    "\\end{split} $$\n",
    "\n",
    " \n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\beta} &= \\sigma_{t-1}^2 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\beta}\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\gamma} &= x_{t-1}^2 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\gamma}\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "Unconditional Expectation of first derivatives of $\\sigma^2$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial \\sigma_{t}^2}{\\partial \\omega}\\right] = \\frac{1}{1-\\beta} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial \\sigma_{t}^2}{\\partial \\alpha}\\right] = \\frac{\\omega + \\gamma \\mu_{x^2}}{(1-\\beta)(1-\\alpha-\\beta)} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial \\sigma_{t}^2}{\\partial \\beta}\\right] = \\frac{\\omega + \\gamma \\mu_{x^2}}{(1-\\beta)(1-\\alpha-\\beta)} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial \\sigma_{t}^2}{\\partial \\gamma}\\right] = \\frac{\\mu_{x^2}}{1-\\beta} $$\n",
    "\n",
    "\n",
    "#### 3.2. Second Derivative to $\\sigma^2$\n",
    "There exist two cases. \n",
    "\n",
    "a). If both parameters $ \\theta_i, \\theta_j $ are not $\\beta$,\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial^2 \\sigma_{t}^2}{\\partial \\theta_i \\theta_j} &= \\beta \\frac{\\partial^2 \\sigma_{t-1}^2}{\\partial \\theta_i \\theta_j} \n",
    "\\end{split} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\theta_i \\partial\\theta_j}\\right] = 0 $$\n",
    "\n",
    "\n",
    "b). If at least one of the parameters $\\theta_i, \\theta_j$ is $\\beta$,\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial^2 \\sigma_{t}^2}{\\partial \\beta \\theta_j} &= \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\theta_j } + \\beta \\frac{\\partial^2 \\sigma_{t-1}^2}{\\partial \\beta \\theta_j} \n",
    "\\end{split} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\beta\\partial\\omega}\\right] = \\frac{1}{(1-\\beta)^2}$$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\beta\\partial\\alpha}\\right] = \\frac{\\omega + \\gamma \\mu_{x^2}}{(1-\\beta)^2(1-\\alpha-\\beta)} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\beta^2}\\right] = \\frac{\\omega + \\gamma \\mu_{x^2}}{(1-\\beta)^2(1-\\alpha-\\beta)} $$\n",
    "\n",
    "$$ E\\left[\\frac{\\partial^2 \\sigma_{t}^2}{\\partial\\beta\\partial\\gamma}\\right] = \\frac{1}{(1-\\beta)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Stationarity of time series.\n",
    "\n",
    "\n",
    "...since the time series is stationary and ergodic, the law of large numbers ensures convergence in probability of the log likelihood function: \n",
    "\n",
    "By the Lindeberg-Feller CLT, as $e_t$ is stationary, \n",
    "$$ \\frac{1}{\\sqrt{n}} s_n(\\theta_0) \\xrightarrow[]{\\text{d}} N(0, I_E(\\theta_0)) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5 Consistency of the MLE.\n",
    "\n",
    "Let the score function be defined as $s(\\theta) = \\frac{\\partial}{\\partial\\theta} l(\\theta)$ and the information $I(\\theta) = \\frac{\\partial^2}{\\partial\\theta^2} l(\\theta)$. \n",
    "\n",
    "Under suitable regularity conditions, and having shown that the time series is stationary and ergodic,\n",
    "\n",
    "$$ \\hat{\\theta} \\xrightarrow[]{\\text{p}} \\theta_0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 6 Asymptotic normality of the MLE.\n",
    "\n",
    "Consider a first order multivariate Taylor expansion of the score function at $\\theta_0$ about $\\hat{\\theta}$:\n",
    "\n",
    "$$ s_n(\\theta_0) \\approx s_n(\\hat{\\theta}) - I_n(\\theta_0) (\\theta_0 - \\hat{\\theta}) $$\n",
    "\n",
    "Since by definition $s_n(\\hat{\\theta}) = 0$,\n",
    "\n",
    "$$ s_n(\\theta_0) = - I_n(\\theta_0) (\\theta_0 - \\hat{\\theta}) $$\n",
    "$$ (\\hat{\\theta} - \\theta_0) = - I_n(\\theta_0)^{-1} s_n(\\theta_0)$$\n",
    "\n",
    "$$ \\sqrt{n}(\\hat{\\theta} - \\theta_0) = - \\sqrt{n} I_n(\\theta_0)^{-1} s_n(\\theta_0) $$\n",
    "\n",
    "Next, it can be shown that $E(s_n(\\theta_0)) = 0$  and $Var(s_n(\\theta_0)) = I_E(\\theta_0)$. Under the law of large numbers, the score function $ s_n(\\theta_0)$ converges in distribution to a multivariate normal distribution $N(0, I_E(\\theta_0))$. \n",
    "\n",
    "Thus, by the Lindeberg-Feller CLT,\n",
    "\n",
    "$$ \\frac{1}{\\sqrt{n}} s_n(\\theta_0) = \\frac{1}{\\sqrt{n}} \\sum_{t=1}^n \\frac{\\partial}{\\partial\\theta} \\log(f(x_i|\\theta)) \\xrightarrow[]{\\text{d}} N(0, I_E(\\theta_0)) $$ \n",
    "\n",
    "and by the law of large numbers,\n",
    "\n",
    "$$\\frac{1}{n} I_n(\\theta_0) = \\frac{1}{n} \\sum_{t=1}^n \\frac{\\partial^2}{\\partial\\theta^2} \\log(f(x_i|\\theta))  \\xrightarrow[]{\\text{p}} I_E(\\theta_0)$$ \n",
    "\n",
    "Therefore by Slutsky's theorem,\n",
    "\n",
    "$$ \\sqrt{n}(\\hat{\\theta} - \\theta_0) \\xrightarrow[]{\\text{d}} I_E(\\theta_0)^{-1} N(0, I(\\theta_0)) = N(0, I_E(\\theta_0)^{-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Asymptotic distribution of the MLE\n",
    "\n",
    "The fisher information is given by \n",
    "\n",
    "$$ I(\\theta) = - \\begin{pmatrix}\n",
    "\\frac{\\partial^2 l}{\\partial \\omega^2} & \\frac{\\partial^2 l}{\\partial \\omega \\partial \\alpha} & \\frac{\\partial^2 l}{\\partial \\omega \\partial \\beta} & \\frac{\\partial^2 l}{\\partial \\omega \\partial \\gamma} \\\\\n",
    "\n",
    "\\frac{\\partial^2 l}{\\partial \\alpha \\partial \\omega} & \\frac{\\partial^2 l}{\\partial \\alpha^2} & \\frac{\\partial^2 l}{\\partial \\alpha \\partial \\beta} & \\frac{\\partial^2 l}{\\partial \\alpha \\partial \\gamma} \\\\\n",
    "\n",
    "\\frac{\\partial^2 l}{\\partial \\beta \\partial \\omega} & \\frac{\\partial^2 l}{\\partial \\beta \\partial \\alpha} & \\frac{\\partial^2 l}{\\partial \\beta^2} & \\frac{\\partial^2 l}{\\partial \\beta \\partial \\gamma} \\\\\n",
    "\n",
    "\\frac{\\partial^2 l}{\\partial \\gamma \\partial \\omega} & \\frac{\\partial^2 l}{\\partial \\gamma \\partial \\alpha} & \\frac{\\partial^2 l}{\\partial \\gamma \\partial \\beta} & \\frac{\\partial^2 l}{\\partial \\gamma^2} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "where each partial derivative is given above.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Assuming consistency (we probably prove this before this part?) / If the regularity conditions hold, the asymptotic distribution of the MLE $\\hat{\\theta}$ converges to a multivariate normal distribution with expected value $\\theta$ and variance $I(\\theta)^{-1}$,i.e. $$\\hat{\\theta} \\sim MVN_d(\\theta_0, I(\\hat{\\theta})^{-1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Consistency of $\\sigma_t^2$ and $r_t^2$\n",
    "to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymtotic distribution of parameters.\n",
    "\n",
    "1. Find first partial derivative for each parameter. For example for $\\gamma$,\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial l}{\\partial \\gamma} &= \\frac{\\partial l}{\\partial \\sigma^2} \\frac{\\partial \\sigma^2}{\\partial \\gamma} \\\\\n",
    "    &= \\sum_{t=1}^T \\frac{x_t^2}{2\\sigma_t^2} (\\frac{e_{t}^2}{\\sigma_t^2} -1)\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "\n",
    "2.  Find second partial derivative for each parameter, to each parameter. For four parameters, we will have 16 partial derivatives. For instance,\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial^2 l}{\\partial \\gamma^2} &= \\frac{\\partial}{\\partial \\sigma^2} (\\frac{\\partial l }{\\partial \\gamma}) \\frac{\\partial \\sigma^2}{\\partial \\gamma} \\\\\n",
    "    &= \\sum_{t=1}^T \\frac{x_t^4}{\\sigma_t^4} (\\frac{1}{2} - \\frac{e_t^2}{\\sigma_t^2})\n",
    "\\end{split} $$\n",
    "\n",
    "3. Find the expectation of the negative of each second partial derivative. Question: do we need to find the expectation, or is the raw form of the second derivative (such as shown in step 2) sufficient?\n",
    "\n",
    "4. This 4x4 matrix then forms our fisher information matrix, $I_E(\\theta)$ where $\\theta$ is the vector of our parameters.\n",
    "\n",
    "5. The asymptotic distribution of our parameters follows a multi variate normal distribution with mean $(\\omega_0, \\alpha_0, \\beta_0, \\gamma_0), $ and variance as the inverse fisher information matrix mentioned in step 3. \n",
    "\n",
    "6. Using the variance, we can find the p value associated with each parameter.\n",
    "\n",
    "---\n",
    "Questions:\n",
    "1. Do we need the regularity conditions for the log likelihood function to hold in order for asymptotic normality and consistency to hold? If so, is this where our assumptions of parameter boundaries and distributions come into place?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for first derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "    Compute the partial derivative of l with respect to alpha.\n",
    "    \n",
    "    Parameters:\n",
    "    x : np.array\n",
    "        The input time series data.\n",
    "    sigma_squared : np.array\n",
    "        The variance values \\( \\sigma_t^2 \\) for each time step.\n",
    "    e : np.array\n",
    "        The error terms \\( e_t \\) for each time step.\n",
    "    \n",
    "    Returns:\n",
    "    float\n",
    "        The computed derivative value.\n",
    "    \"\"\"\n",
    "\n",
    "### a.\n",
    "def partial_l_gamma(x, sigma_squared, e):\n",
    "    T = len(x)\n",
    "    derivative = np.sum((x**2 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n",
    "\n",
    "### b.\n",
    "def partial_l_omega(sigma_squared, e):\n",
    "    T = len(x)\n",
    "    derivative = np.sum((1 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n",
    "\n",
    "### c.\n",
    "def partial_l_alpha(x, sigma_squared, e):\n",
    "    T = len(x)\n",
    "    derivative = np.sum((x**2 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n",
    "\n",
    "### d.\n",
    "def partial_l_beta(x, sigma_squared, e):\n",
    "    sigma_squared_tminus1 = sigma_squared.shift(1)\n",
    "    T = len(x)\n",
    "    derivative = np.sum((sigma_squared_tminus1 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proving consistency: <a href='https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Consistency'>Consistency</a>\n",
    "Regulartiy conditions: <a href='https://en.wikipedia.org/wiki/Fisher_information#Regularity_conditions'>Regularity, Fisher Information</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of consistency\n",
    "1. Ergodicity,\n",
    "2. Stationarity,\n",
    "3. $\\theta_0$ is not on the boundary of the parameter space.\n",
    "\n",
    "If a time series $X_t$ is stationary and erogdic, then \n",
    "$$\\frac{1}{T} \\sum_{t=1}^T X_t \\xrightarrow[]{\\text{p}} \\mu $$ \n",
    "where  $\\mu = E[X_t] < \\infty$.\n",
    "\n",
    "### Ergodic Theorem\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Notes\n",
    "$x_t$ stationary and ergodic => $x_t^2$ stationary and ergodic\n",
    "\n",
    "\n",
    "### Backlog:\n",
    "- Testing raw data for stationary and ergodicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-125.72244114788363"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "s0 = 90\n",
    "u = 1.2\n",
    "d = 0.8\n",
    "r = 0.1\n",
    "K = 100\n",
    "\n",
    "s0 * (d - u*np.e**(-1*r)) - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7036.015762647621"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "90**2 * 1.2 * 0.8 * np.e**(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.2 * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
