{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our GARCH(1,1) model\n",
    "\n",
    "Mean Model\n",
    "$$ r_t = \\sigma_t z_t $$ \n",
    "$$z_t | \\mathcal{F}_{t-1} \\sim N(0, 1)$$\n",
    "where $e_t = \\sigma_t z_t$\n",
    "\n",
    "Volatility Model \n",
    "$$ \\sigma_t^2 = \\omega +  \\alpha e_{t-1}^2 + \\beta \\sigma_{t-1}^2 + \n",
    "    \\gamma x_t^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log likelihood\n",
    " $$ l(\\omega, \\alpha, \\beta, \\gamma) =  \\sum_{t=1}^T {\\frac{1}{2} ({-\\log{2\\pi} -\\log{\\sigma_t^2} - \\frac{e_t^2}{\\sigma_t^2}})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymtotic distribution of parameters.\n",
    "\n",
    "1. Find first partial derivative for each parameter. For example for $\\gamma$,\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial l}{\\partial \\gamma} &= \\frac{\\partial l}{\\partial \\sigma^2} \\frac{\\partial \\sigma^2}{\\partial \\gamma} \\\\\n",
    "    &= \\sum_{t=1}^T \\frac{x_t^2}{2\\sigma_t^2} (\\frac{e_{t}^2}{\\sigma_t^2} -1)\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "\n",
    "2.  Find second partial derivative for each parameter, to each parameter. For four parameters, we will have 16 partial derivatives. For instance,\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial^2 l}{\\partial \\gamma^2} &= \\frac{\\partial}{\\partial \\sigma^2} (\\frac{\\partial l }{\\partial \\gamma}) \\frac{\\partial \\sigma^2}{\\partial \\gamma} \\\\\n",
    "    &= \\sum_{t=1}^T \\frac{x_t^4}{\\sigma_t^4} (\\frac{1}{2} - \\frac{e_t^2}{\\sigma_t^2})\n",
    "\\end{split} $$\n",
    "\n",
    "3. Find the expectation of the negative of each second partial derivative. Question: do we need to find the expectation, or is the raw form of the second derivative (such as shown in step 2) sufficient?\n",
    "\n",
    "4. This 4x4 matrix then forms our fisher information matrix, $I_E(\\theta)$ where $\\theta$ is the vector of our parameters.\n",
    "\n",
    "5. The asymptotic distribution of our parameters follows a multi variate normal distribution with mean $(\\omega_0, \\alpha_0, \\beta_0, \\gamma_0), $ and variance as the inverse fisher information matrix mentioned in step 3. \n",
    "\n",
    "6. Using the variance, we can find the p value associated with each parameter.\n",
    "\n",
    "---\n",
    "Questions:\n",
    "1. Do we need the regularity conditions for the log likelihood function to hold in order for asymptotic normality and consistency to hold? If so, is this where our assumptions of parameter boundaries and distributions come into place?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Derivative to $\\sigma^2$\n",
    "\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\omega} &= 1 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\omega} \n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\alpha} &= e_{t-1}^2 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\alpha}\n",
    "\\end{split} $$\n",
    "\n",
    " \n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\beta} &= \\sigma_{t-1}^2 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\beta}\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial \\sigma_{t}^2}{\\partial \\gamma} &= x_{t-1}^2 + \\beta \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\gamma}\n",
    "\\end{split} $$\n",
    "\n",
    "\n",
    "## Second Derivative to $\\sigma^2$\n",
    "Two cases. \n",
    "1. If both parameters $ \\theta_i, \\theta_j $ are not $\\beta$,\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial^2 \\sigma_{t}^2}{\\partial \\theta_i \\theta_j} &= \\beta \\frac{\\partial^2 \\sigma_{t-1}^2}{\\partial \\theta_i \\theta_j} \n",
    "\\end{split} $$\n",
    "\n",
    "2. For any two parameters $\\theta_i, \\theta_j$ where $\\theta_i = \\beta$,\n",
    "$$ \\begin{split}\n",
    "    \\frac{\\partial^2 \\sigma_{t}^2}{\\partial \\theta_i \\theta_j} &= \\frac{\\partial \\sigma_{t-1}^2}{\\partial \\theta_j } + \\beta \\frac{\\partial^2 \\sigma_{t-1}^2}{\\partial \\theta_i \\theta_j} \n",
    "\\end{split} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Derivatives of Log likelihood\n",
    "\n",
    "Let our model be parameterised by $\\theta = (\\omega, \\alpha, \\beta, \\gamma)^T$.\n",
    "\n",
    "For any parameters $\\theta_1, \\theta_2$, \n",
    "\n",
    "$$ \\begin{split}\n",
    "\\frac{\\partial }{\\partial \\theta_1 \\partial \\theta_2} l(\\theta) &= - \\frac{1}{2} \\sum_{t=1}^T\\left( \\frac{\\partial^2 \\sigma_t^2}{\\partial \\theta_1 \\partial \\theta_2} (\\frac{1}{\\sigma_t^2} - \\frac{e_t^2}{\\sigma_t^4}) + \n",
    "\n",
    "\\frac{\\partial \\sigma_t^2}{\\partial \\theta_1} \\frac{\\partial \\sigma_t^2}{\\partial \\theta_2} (\\frac{2e_t^2}{\\sigma_t^6} - \\frac{1}{\\sigma_t^4})\\right) \n",
    "\n",
    "\\end{split} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic distribution of the MLE\n",
    "\n",
    "Let $\\theta = (\\omega, \\alpha, \\beta, \\gamma)^T$ be our parameter vector. \n",
    "\n",
    "The fisher information is given by \n",
    "\n",
    "$$ I(\\theta) = - \\begin{pmatrix}\n",
    "\\frac{\\partial^2 l}{\\partial \\omega^2} & \\frac{\\partial^2 l}{\\partial \\omega \\partial \\alpha} & \\frac{\\partial^2 l}{\\partial \\omega \\partial \\beta} & \\frac{\\partial^2 l}{\\partial \\omega \\partial \\gamma} \\\\\n",
    "\n",
    "\\frac{\\partial^2 l}{\\partial \\alpha \\partial \\omega} & \\frac{\\partial^2 l}{\\partial \\alpha^2} & \\frac{\\partial^2 l}{\\partial \\alpha \\partial \\beta} & \\frac{\\partial^2 l}{\\partial \\alpha \\partial \\gamma} \\\\\n",
    "\n",
    "\\frac{\\partial^2 l}{\\partial \\beta \\partial \\omega} & \\frac{\\partial^2 l}{\\partial \\beta \\partial \\alpha} & \\frac{\\partial^2 l}{\\partial \\beta^2} & \\frac{\\partial^2 l}{\\partial \\beta \\partial \\gamma} \\\\\n",
    "\n",
    "\\frac{\\partial^2 l}{\\partial \\gamma \\partial \\omega} & \\frac{\\partial^2 l}{\\partial \\gamma \\partial \\alpha} & \\frac{\\partial^2 l}{\\partial \\gamma \\partial \\beta} & \\frac{\\partial^2 l}{\\partial \\gamma^2} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "where each partial derivative is given above.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Assuming consistency (we probably prove this before this part?), the asymptotic distribution of the MLE $\\hat{\\theta}$ converges to a multivariate normal distribution with expected value $\\theta$ and variance $I(\\theta)^{-1}$,i.e. $$\\hat{\\theta} \\sim MVN_d(\\theta_0, I(\\hat{\\theta})^{-1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for first derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "    Compute the partial derivative of l with respect to alpha.\n",
    "    \n",
    "    Parameters:\n",
    "    x : np.array\n",
    "        The input time series data.\n",
    "    sigma_squared : np.array\n",
    "        The variance values \\( \\sigma_t^2 \\) for each time step.\n",
    "    e : np.array\n",
    "        The error terms \\( e_t \\) for each time step.\n",
    "    \n",
    "    Returns:\n",
    "    float\n",
    "        The computed derivative value.\n",
    "    \"\"\"\n",
    "\n",
    "### a.\n",
    "def partial_l_gamma(x, sigma_squared, e):\n",
    "    T = len(x)\n",
    "    derivative = np.sum((x**2 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n",
    "\n",
    "### b.\n",
    "def partial_l_omega(sigma_squared, e):\n",
    "    T = len(x)\n",
    "    derivative = np.sum((1 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n",
    "\n",
    "### c.\n",
    "def partial_l_alpha(x, sigma_squared, e):\n",
    "    T = len(x)\n",
    "    derivative = np.sum((x**2 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n",
    "\n",
    "### d.\n",
    "def partial_l_beta(x, sigma_squared, e):\n",
    "    sigma_squared_tminus1 = sigma_squared.shift(1)\n",
    "    T = len(x)\n",
    "    derivative = np.sum((sigma_squared_tminus1 / (2 * sigma_squared)) * ((e**2 / sigma_squared) - 1))\n",
    "    return derivative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proving consistency: <a href='https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Consistency'>Consistency</a>\n",
    "Regulartiy conditions: <a href='https://en.wikipedia.org/wiki/Fisher_information#Regularity_conditions'>Regularity, Fisher Information</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of consistency\n",
    "1. Ergodicity,\n",
    "2. Stationarity,\n",
    "3. $\\theta_0$ is not on the boundary of the parameter space.\n",
    "\n",
    "If a time series $X_t$ is stationary and erogdic, then \n",
    "$$\\frac{1}{T} \\sum_{t=1}^T X_t \\xrightarrow[]{\\text{p}} \\mu $$ \n",
    "where  $\\mu = E[X_t] < \\infty$.\n",
    "\n",
    "### Ergodic Theorem\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Notes\n",
    "$x_t$ stationary and ergodic => $x_t^2$ stationary and ergodic\n",
    "\n",
    "\n",
    "### Backlog:\n",
    "- Testing raw data for stationary and ergodicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-125.72244114788363"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "s0 = 90\n",
    "u = 1.2\n",
    "d = 0.8\n",
    "r = 0.1\n",
    "K = 100\n",
    "\n",
    "s0 * (d - u*np.e**(-1*r)) - K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7036.015762647621"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "90**2 * 1.2 * 0.8 * np.e**(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.2 * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
